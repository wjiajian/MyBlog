---
slug: rag-stage3
title: Agent å­¦ä¹ ï¼šå·¥ç¨‹åŒ–ä¼˜åŒ–ä¸é«˜çº§ RAG æŠ€æœ¯
year: 2025
date: 2025-12-08
description: ä»è·‘é€šä»£ç åˆ°è§£å†³çœŸå®ä¸–ç•Œçš„å¤æ‚åœºæ™¯ã€‚æœ¬ç¯‡æ·±å…¥è®²è§£å›ºå®šé•¿åº¦ã€å¥å­çº§ã€é€’å½’ç­‰å¤šç§åˆ‡åˆ†ç­–ç•¥çš„é€‚ç”¨åœºæ™¯ï¼Œç»“åˆBM25å…³é”®è¯æ£€ç´¢ä¸è¯­ä¹‰æ£€ç´¢æ„å»ºæ··åˆæ£€ç´¢ç³»ç»Ÿï¼Œå¹¶ä»‹ç»é‡æ’åºæŠ€æœ¯ä¼˜åŒ–Top-Kæ£€ç´¢ç»“æœï¼Œæœ€ç»ˆæ„å»ºç”Ÿäº§çº§RAGç³»ç»Ÿã€‚
coverImage: https://images.unsplash.com/photo-1677442136019-21780ecad995?q=80&w=1000&auto=format&fit=crop
tags:
  - Agent
  - RAG
  - æ–‡æœ¬åˆ‡åˆ†
  - æ··åˆæ£€ç´¢
categories: ç¬”è®°
type: tech
---

> **æ ¸å¿ƒç†å¿µ**ï¼šä»"è·‘é€šä»£ç "åˆ°"è§£å†³çœŸå®ä¸–ç•Œçš„å¤æ‚è„æ´»"ï¼ŒæŒæ¡ç”Ÿäº§çº§ RAG ç³»ç»Ÿçš„å·¥ç¨‹åŒ–æ·±åº¦ã€‚

---

## ğŸ“‹ å®éªŒç›®æ ‡

1. **æŒæ¡é«˜çº§åˆ‡åˆ†ç­–ç•¥**ï¼šç†è§£ä¸åŒåˆ‡åˆ†æ–¹æ³•çš„é€‚ç”¨åœºæ™¯å’Œæ•ˆæœå·®å¼‚
2. **ä¼˜åŒ–æ£€ç´¢å‡†ç¡®åº¦**ï¼šå­¦ä¹ æ··åˆæ£€ç´¢ã€é‡æ’åºç­‰å…ˆè¿›æŠ€æœ¯
3. **ç®¡ç†ä¸Šä¸‹æ–‡çª—å£**ï¼šè§£å†³é•¿æ–‡æ¡£ã€æˆæœ¬æ§åˆ¶ç­‰å·¥ç¨‹éš¾é¢˜
4. **å®æˆ˜é¡¹ç›®**ï¼šæ„å»º"æ™ºèƒ½æŠ•åæŠ¥å‘Šåˆ†æåŠ©æ‰‹"ï¼Œå¤„ç†çœŸå®å¤æ‚åœºæ™¯
5. **æ€§èƒ½è°ƒä¼˜**ï¼šå»ºç«‹è¯„ä¼°ä½“ç³»ï¼ŒæŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½

---

## ğŸ› ï¸ å®éªŒç¯å¢ƒå‡†å¤‡

### å‡çº§ä¾èµ–å®‰è£…

```bash
# æ¿€æ´»ç¬¬äºŒé˜¶æ®µç¯å¢ƒ
# rag_env\Scripts\activate  # Windows
# source rag_env/bin/activate  # Linux/Mac

# å®‰è£…é«˜çº§ä¾èµ–
pip install matplotlib seaborn scikit-learn
pip install rank_bm25  # å…³é”®è¯æ£€ç´¢
pip install sentence-transformers[visualization]  # å¯è§†åŒ–å·¥å…·
pip install umap-learn  # é™ç»´å¯è§†åŒ–
pip install jieba  # ä¸­æ–‡åˆ†è¯

# å¯é€‰ï¼šé«˜çº§å‘é‡æ•°æ®åº“
pip install qdrant-client  # Qdrant å‘é‡æ•°æ®åº“
pip install pymilvus  # Milvus å‘é‡æ•°æ®åº“

# æ–‡æ¡£è§£æå·¥å…·
pip install unstructured
pip install pymupdf  # PDF å¤„ç†
pip install pandas openpyxl  # è¡¨æ ¼å¤„ç†
```

### ç¯å¢ƒéªŒè¯


```python
import sys

def test_dependencies():
    """æµ‹è¯•Stage 3ç¯å¢ƒä¾èµ–"""
    deps = {
        'sentence_transformers': 'sentence-transformers',
        'faiss': 'faiss-cpu',
        'sklearn': 'scikit-learn',
        'matplotlib': 'matplotlib',
        'seaborn': 'seaborn',
        'rank_bm25': 'rank_bm25',
        'umap': 'umap-learn',
        'unstructured': 'unstructured',
        'fitz': 'pymupdf',
        'pandas': 'pandas',
        'jieba': 'jieba',
    }

    print("=" * 60)
    print("Stage 3 ä¾èµ–æ£€æŸ¥")
    print("=" * 60)

    missing = []
    for name, package in deps.items():
        try:
            module = __import__(name)
            version = getattr(module, '__version__', 'unknown')
            print(f"âœ… {package:20s} v{version}")
        except ImportError:
            print(f"âŒ {package:20s} æœªå®‰è£…")
            missing.append(package)

    if missing:
        print(f"\nâš ï¸  è¯·å®‰è£…ç¼ºå¤±ä¾èµ–: pip install {' '.join(missing)}")
        return False
    else:
        print("\nâœ… æ‰€æœ‰ä¾èµ–å·²å°±ç»ªï¼")
        return True

if __name__ == "__main__":
    success = test_dependencies()
    sys.exit(0)
```

---

## å®éªŒä¸€ï¼šé«˜çº§åˆ‡åˆ†ç­–ç•¥ (Advanced Chunking)

### å®éªŒç›®æ ‡

æ·±å…¥ç†è§£ä¸åŒåˆ‡åˆ†ç­–ç•¥å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“ï¼ŒæŒæ¡ä½•æ—¶ä½¿ç”¨ä½•ç§åˆ‡åˆ†æ–¹æ³•ã€‚

### ç†è®ºåŸºç¡€

#### 1. åˆ‡åˆ†ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | å®ç°éš¾åº¦ | æ£€ç´¢æ•ˆæœ | ä¸Šä¸‹æ–‡ä¿æŒ | è®¡ç®—æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|----------|----------|------------|----------|----------|
| å›ºå®šé•¿åº¦ | â­ | â­â­ | â­ | â­â­â­â­â­ | ç®€å•æ–‡æ¡£ã€æ—¥å¿— |
| å¥å­çº§ | â­â­ | â­â­â­ | â­â­ | â­â­â­â­ | è®ºæ–‡ã€æ–°é—» |
| æ®µè½çº§ | â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | å¸¸è§„æ–‡æ¡£ |
| é€’å½’åˆ‡åˆ† | â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­ | é€šç”¨åœºæ™¯ |
| è¯­ä¹‰åˆ‡åˆ† | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­ | å¤æ‚æ–‡æ¡£ |

#### 2. Overlap çª—å£ç­–ç•¥

**é—®é¢˜**ï¼šåˆ‡åˆ†æ—¶ä¸¢å¤±è·¨å—ä¸Šä¸‹æ–‡ä¿¡æ¯
**è§£å†³æ–¹æ¡ˆ**ï¼šç›¸é‚»å—ä¹‹é—´ä¿ç•™é‡å åŒºåŸŸ

```
å—1: [AAAAAAAAAA]BBBBBBBBB
å—2:          [CCCCCCCCCC]DDDDDDDDD
      â†‘ é‡å åŒºåŸŸ â†‘

æœ€ä½³å®è·µï¼š
- é‡å æ¯”ä¾‹: 10-20%
- é‡å è¿‡å°‘: ä¸¢å¤±è¯­ä¹‰
- é‡å è¿‡å¤š: æµªè´¹å­˜å‚¨
```

### å®éªŒæ­¥éª¤

#### Step 1: å®ç°å¤šç§åˆ‡åˆ†ç­–ç•¥


```python
import re
import numpy as np
from typing import List, Tuple, Optional, Generator
from dataclasses import dataclass

@dataclass
class Chunk:
    """æ–‡æœ¬å—æ•°æ®ç»“æ„"""
    __slots__ = ['text', 'start', 'end', 'chunk_id', 'token_count']  # ä¼˜åŒ–å†…å­˜å ç”¨
    text: str
    start: int
    end: int
    chunk_id: int
    token_count: int

class AdvancedChunker:
    """é«˜çº§æ–‡æœ¬åˆ‡åˆ†å™¨ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰"""

    def __init__(self):
        self.chunk_id_counter = 0
        # é¢„ç¼–è¯‘æ­£åˆ™ï¼Œæé«˜æ€§èƒ½
        self.sentence_split_pattern = re.compile(r'[ã€‚ï¼ï¼Ÿï¼›.!?;]')

    def count_tokens(self, text: str) -> int:
        """ç®€å•tokenè®¡æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼‰"""
        return len(text)

    def _create_chunk(self, text: str, start: int, end: int) -> Chunk:
        """å†…éƒ¨è¾…åŠ©æ–¹æ³•ï¼šå¿«é€Ÿåˆ›å»ºChunk"""
        chunk = Chunk(
            text=text,
            start=start,
            end=end,
            chunk_id=self.chunk_id_counter,
            token_count=len(text)
        )
        self.chunk_id_counter += 1
        return chunk

    def chunk_by_fixed_length(
        self,
        text: str,
        chunk_size: int = 200,
        overlap: int = 20
    ) -> List[Chunk]:
        """
        å›ºå®šé•¿åº¦åˆ‡åˆ†ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨rangeæ­¥è¿›ï¼Œå‡å°‘åˆ‡ç‰‡å¼€é”€ï¼‰
        """
        chunks = []
        text_len = len(text)
        
        # æ­¥è¿›å¤„ç†ï¼Œé¿å…whileå¾ªç¯ä¸­çš„é‡å¤è®¡ç®—
        step = chunk_size - overlap if overlap > 0 else chunk_size
        if step <= 0: step = 1 # é˜²æ­¢æ­»å¾ªç¯
        
        for start in range(0, text_len, step):
            end = min(start + chunk_size, text_len)
            # å¦‚æœæ˜¯æœ€åä¸€æ®µä¸”ç”±overlapå¯¼è‡´é‡å¤ï¼Œéœ€è¦åˆ¤æ–­ï¼ˆåŸé€»è¾‘ä¿ç•™ï¼‰
            if start > 0 and end == text_len and (start + overlap) > text_len:
                 # è¿™ç§æƒ…å†µä¸‹å¯èƒ½ä¼šäº§ç”Ÿéå¸¸çŸ­çš„å°¾å·´æˆ–è€…é‡å¤ï¼Œæ ¹æ®åŸé€»è¾‘ç›´æ¥åˆ‡åˆ†å³å¯
                 pass

            chunk_text = text[start:end]
            chunks.append(self._create_chunk(chunk_text, start, end))
            
            if end == text_len:
                break

        return chunks

    def chunk_by_sentences(self, text: str, min_chunk_size: int = 50) -> List[Chunk]:
        """
        å¥å­çº§åˆ‡åˆ†ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨åˆ—è¡¨ç¼“å­˜æ›¿ä»£å­—ç¬¦ä¸²æ‹¼æ¥ï¼‰
        """
        # ä½¿ç”¨ split åˆ‡åˆ†ï¼Œæ³¨æ„ï¼šåŸé€»è¾‘ä¼šä¸¢å¤±åˆ†éš”ç¬¦ï¼Œè¿™é‡Œä¿æŒåŸé€»è¾‘è¡Œä¸º
        # ä¹Ÿå°±æ˜¯åŸé€»è¾‘ä¼šæŠŠ 'ã€‚' ä¸¢æ‰ï¼Œç„¶åæ‰‹åŠ¨åŠ å›æ¥
        sentences = self.sentence_split_pattern.split(text)

        chunks = []
        current_buffer = []  # ä½¿ç”¨åˆ—è¡¨ä»£æ›¿å­—ç¬¦ä¸²+=
        current_len = 0
        
        # è¿½è¸ªæ–‡æœ¬ä½ç½®
        current_pos = 0 
        chunk_start = 0

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # åŸé€»è¾‘ï¼šæ¯ä¸ªå¥å­æ‰‹åŠ¨åŠ å¥å·
            sentence_with_punct = sentence + "ã€‚"
            sent_len = len(sentence_with_punct)

            # å¦‚æœåŠ ä¸Šè¿™ä¸ªå¥å­ä¼šå¤ªé•¿ï¼Œä¿å­˜å½“å‰chunk
            if current_len + sent_len > 300 and current_buffer:
                text_content = "".join(current_buffer)
                chunks.append(self._create_chunk(
                    text=text_content,
                    start=chunk_start,
                    end=chunk_start + len(text_content)
                ))
                
                # é‡ç½®Buffer
                current_buffer = [sentence_with_punct]
                current_len = sent_len
                # æ›´æ–°ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè¿‘ä¼¼å€¼ï¼Œå› ä¸ºåŸé€»è¾‘é€šè¿‡stripå’ŒåŠ å¥å·ä¿®æ”¹äº†æ–‡æœ¬ï¼Œæ— æ³•ç²¾ç¡®å¯¹åº”åŸæ–‡ä½ç½®ï¼‰
                # è¿™é‡Œä¸ºäº†æ€§èƒ½å’Œé€»è¾‘è¿ç»­æ€§ï¼Œæˆ‘ä»¬ç´¯åŠ é•¿åº¦
                chunk_start += len(text_content) 
            else:
                current_buffer.append(sentence_with_punct)
                current_len += sent_len

        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_buffer:
            text_content = "".join(current_buffer)
            chunks.append(self._create_chunk(
                text=text_content,
                start=chunk_start,
                end=chunk_start + len(text_content)
            ))

        return chunks

    def chunk_by_recursive(
        self,
        text: str,
        chunk_size: int = 300,
        overlap: int = 30,
        separators: List[str] = None
    ) -> List[Chunk]:
        """
        é€’å½’åˆ‡åˆ†ï¼ˆä¼˜åŒ–ï¼šç”Ÿæˆå™¨æ¨¡å¼ + åˆ—è¡¨ç¼“å­˜ï¼Œå¤§å¹…é™ä½å†…å­˜å’ŒCPUæ¶ˆè€—ï¼‰
        """
        if separators is None:
            separators = ['\n\n', '\n', 'ã€‚', 'ï¼›', 'ï¼Œ', '']

        # å†…éƒ¨ç”Ÿæˆå™¨ï¼šæµå¼äº§ç”Ÿæ–‡æœ¬ç‰‡æ®µï¼Œé¿å…æ„å»ºå·¨å¤§çš„ä¸­é—´åˆ—è¡¨
        def _recursive_split_gen(text_segment: str, seps: List[str]) -> Generator[str, None, None]:
            if not seps or len(text_segment) <= chunk_size:
                if text_segment:
                    yield text_segment
                return

            separator = seps[0]
            # åªæœ‰å½“separatorå­˜åœ¨æ—¶æ‰splitï¼Œå¦åˆ™ç›´æ¥ç”±splitå¤„ç†ï¼ˆsplitç©ºä¸²ä¼šæŠ¥é”™ï¼‰
            if separator:
                parts = text_segment.split(separator)
            else:
                parts = [text_segment]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹ä¸€çº§åˆ†å‰²
            # ä¼˜åŒ–ï¼šä¸éœ€è¦æ£€æŸ¥æ‰€æœ‰partï¼Œåªè¦å‘ç°æœ‰å¤§çš„å°±å¿…é¡»é€’å½’ï¼ˆæˆ–è€…å…¨éƒ¨é€’å½’ï¼‰
            # åŸé€»è¾‘æ˜¯ï¼šå¦‚æœ any part > chunk_sizeï¼Œåˆ™å…¨éƒ¨è¿›å…¥ä¸‹ä¸€çº§ã€‚
            # è¿™å…¶å®æ¯”è¾ƒä½æ•ˆï¼Œé€šå¸¸æ˜¯åˆ†æ²»ï¼šå¤§çš„ç»§ç»­åˆ†ï¼Œå°çš„ä¿ç•™ã€‚
            # ä½†ä¸ºäº†ä¿æŒå’ŒåŸä»£ç é€»è¾‘ä¸€è‡´ï¼ˆ"å¦‚æœåˆ†å‰²åçš„éƒ¨åˆ†éƒ½å¤ªå¤§..."ï¼‰ï¼Œæˆ‘ä»¬ä¿ç•™åŸæ„ï¼š
            # åŸé€»è¾‘ï¼šif any(...) -> use next separator for ALL text.
            
            if any(len(p) > chunk_size for p in parts):
                yield from _recursive_split_gen(text_segment, seps[1:])
            else:
                for part in parts:
                    if len(part) > chunk_size:
                        # ç†è®ºä¸Šä¸Šé¢anyå·²ç»æ‹¦æˆªäº†ï¼Œä½†ä¸ºäº†å®‰å…¨ä¿ç•™
                        yield from _recursive_split_gen(part, seps[1:])
                    else:
                        yield part

        # å¼€å§‹å¤„ç†
        # è¿™é‡Œçš„partsç°åœ¨æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä¸ä¼šä¸€æ¬¡æ€§å¡æ­»å†…å­˜
        parts_gen = _recursive_split_gen(text, separators)

        chunks = []
        current_buffer = []
        current_len = 0
        
        # ç®€å•çš„overlapå¤„ç†ç¼“å­˜
        # åŸé€»è¾‘ overlap æ¯”è¾ƒç®€å•ï¼Œæ˜¯å– current_chunk çš„ååŠéƒ¨åˆ†
        # ä¸ºäº†é«˜æ•ˆï¼Œæˆ‘ä»¬åªåœ¨åˆ‡åˆ†æ—¶è®¡ç®— overlap
        
        for part in parts_gen:
            part = part.strip()
            if not part:
                continue

            part_len = len(part)
            
            if current_len + part_len > chunk_size and current_buffer:
                # 1. ç”Ÿæˆå½“å‰å—
                text_content = "".join(current_buffer)
                chunks.append(self._create_chunk(
                    text=text_content,
                    start=0, # é€’å½’åˆ‡åˆ†æ¯”è¾ƒå¤æ‚ï¼ŒåŸä»£ç ä¹Ÿæ˜¯0ï¼Œè¿™é‡Œæš‚ä¿æŒä¸º0æˆ–éœ€è¦é‡å†™å¤§é‡é€»è¾‘è¿½è¸ªoffset
                    end=len(text_content)
                ))

                # 2. å¤„ç† Overlap
                # ä» text_content æœ«å°¾å– overlap
                overlap_text = text_content[-overlap:] if overlap > 0 else ""
                
                # 3.ä»¥æ­¤ä½œä¸ºæ–°å—çš„å¼€å§‹
                current_buffer = [overlap_text, part] if overlap_text else [part]
                current_len = len(overlap_text) + part_len
            else:
                current_buffer.append(part)
                current_len += part_len

        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_buffer:
            text_content = "".join(current_buffer)
            chunks.append(self._create_chunk(
                text=text_content,
                start=0,
                end=len(text_content)
            ))

        return chunks

# æµ‹è¯•ä¸åŒåˆ‡åˆ†ç­–ç•¥
if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open('../Stage_2/anthropic.txt', 'r', encoding='utf-8') as f:
        test_text = f.read()[:2000]  # å–å‰2000å­—ç¬¦æµ‹è¯•

    chunker = AdvancedChunker()

    # ç­–ç•¥1: å›ºå®šé•¿åº¦
    fixed_chunks = chunker.chunk_by_fixed_length(test_text, 200, 20)
    print(f"å›ºå®šé•¿åº¦åˆ‡åˆ†: {len(fixed_chunks)} ä¸ªå—")

    # ç­–ç•¥2: å¥å­çº§
    sentence_chunks = chunker.chunk_by_sentences(test_text)
    print(f"å¥å­çº§åˆ‡åˆ†: {len(sentence_chunks)} ä¸ªå—")

    # ç­–ç•¥3: é€’å½’åˆ‡åˆ†
    recursive_chunks = chunker.chunk_by_recursive(test_text, 300, 30)
    print(f"é€’å½’åˆ‡åˆ†: {len(recursive_chunks)} ä¸ªå—")

    # åˆ†æåˆ‡åˆ†æ•ˆæœ
    print("\n=== åˆ‡åˆ†æ•ˆæœåˆ†æ ===")
    for name, chunks in [
        ("å›ºå®šé•¿åº¦", fixed_chunks),
        ("å¥å­çº§", sentence_chunks),
        ("é€’å½’åˆ‡åˆ†", recursive_chunks)
    ]:
        avg_tokens = np.mean([c.token_count for c in chunks])
        std_tokens = np.std([c.token_count for c in chunks])
        print(f"\n{name}:")
        print(f"  å¹³å‡Tokenæ•°: {avg_tokens:.1f} Â± {std_tokens:.1f}")
        print(f"  å—æ•°é‡: {len(chunks)}")
        # å¢åŠ åˆ¤æ–­ï¼Œé˜²æ­¢ç©ºåˆ—è¡¨æŠ¥é”™
        if chunks:
            print(f"  ç¤ºä¾‹å—: {chunks[0].text[:100]}...")
```

    å›ºå®šé•¿åº¦åˆ‡åˆ†: 11 ä¸ªå—
    å¥å­çº§åˆ‡åˆ†: 8 ä¸ªå—
    é€’å½’åˆ‡åˆ†: 9 ä¸ªå—
    
    === åˆ‡åˆ†æ•ˆæœåˆ†æ ===
    
    å›ºå®šé•¿åº¦:
      å¹³å‡Tokenæ•°: 200.0 Â± 0.0
      å—æ•°é‡: 11
      ç¤ºä¾‹å—: Anthropicæ˜¯ç”±OpenAIå‰é«˜å±‚ä¸¹å¦®æ‹‰Â·é˜¿è«è¿ªï¼ˆDaniela Amodeiï¼‰å’Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ªï¼ˆDario Amodeiï¼‰äº2021å¹´åˆ›ç«‹çš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œæ€»éƒ¨ä½äºç¾å›½åŠ å·æ—§é‡‘å±±ï¼Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ª...
    
    å¥å­çº§:
      å¹³å‡Tokenæ•°: 248.9 Â± 59.3
      å—æ•°é‡: 8
      ç¤ºä¾‹å—: Anthropicæ˜¯ç”±OpenAIå‰é«˜å±‚ä¸¹å¦®æ‹‰Â·é˜¿è«è¿ªï¼ˆDaniela Amodeiï¼‰å’Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ªï¼ˆDario Amodeiï¼‰äº2021å¹´åˆ›ç«‹çš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œæ€»éƒ¨ä½äºç¾å›½åŠ å·æ—§é‡‘å±±ï¼Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ª...
    
    é€’å½’åˆ‡åˆ†:
      å¹³å‡Tokenæ•°: 244.0 Â± 45.2
      å—æ•°é‡: 9
      ç¤ºä¾‹å—: Anthropicæ˜¯ç”±OpenAIå‰é«˜å±‚ä¸¹å¦®æ‹‰Â·é˜¿è«è¿ªï¼ˆDaniela Amodeiï¼‰å’Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ªï¼ˆDario Amodeiï¼‰äº2021å¹´åˆ›ç«‹çš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œæ€»éƒ¨ä½äºç¾å›½åŠ å·æ—§é‡‘å±±ï¼Œè¾¾é‡Œå¥¥Â·é˜¿è«è¿ª...
    

#### Step 2: åˆ‡åˆ†æ•ˆæœè¯„ä¼°


```python
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥è§£å†³è­¦å‘Šé—®é¢˜
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    try:
        # æ–¹æ³•1: ä½¿ç”¨ç³»ç»Ÿå­—ä½“
        import matplotlib
        system_fonts = matplotlib.font_manager.get_font_names()
        
        # å¸¸è§ä¸­æ–‡å­—ä½“åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        chinese_fonts = [
            'SimHei',       # Windows é»‘ä½“
            'Microsoft YaHei',  # Windows å¾®è½¯é›…é»‘
            'STHeiti',      # Mac é»‘ä½“
            'STXihei',      # Mac ç»†é»‘
            'STKaiti',      # Mac æ¥·ä½“
            'STSong',       # Mac å®‹ä½“
            'STFangsong',   # Mac ä»¿å®‹
            'SimSun',       # Windows å®‹ä½“
            'NSimSun',      # Windows æ–°å®‹ä½“
            'FangSong',     # Windows ä»¿å®‹
            'KaiTi',        # Windows æ¥·ä½“
            'Arial Unicode MS',  # é€šç”¨å­—ä½“
            'DejaVu Sans'   # Linux é»˜è®¤
        ]
        
        # æ£€æŸ¥ç³»ç»Ÿå¯ç”¨çš„ä¸­æ–‡å­—ä½“
        available_fonts = []
        for font in chinese_fonts:
            if font in system_fonts:
                available_fonts.append(font)
        
        if available_fonts:
            # è®¾ç½®å­—ä½“
            plt.rcParams['font.sans-serif'] = available_fonts
            plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
            
            # æ¸…é™¤matplotlibç¼“å­˜
            matplotlib.font_manager._rebuild()
            
            print(f"âœ“ å·²è®¾ç½®ä¸­æ–‡å­—ä½“: {available_fonts[0]}")
            return True
        else:
            # æ–¹æ³•2: å°è¯•ä¸‹è½½ä¸­æ–‡å­—ä½“
            print("âš  æœªæ‰¾åˆ°ç³»ç»Ÿä¸­æ–‡å­—ä½“ï¼Œå°è¯•ä¸‹è½½...")
            return download_chinese_font()
            
    except Exception as e:
        print(f"å­—ä½“è®¾ç½®å¤±è´¥: {e}")
        return False

def download_chinese_font():
    """ä¸‹è½½ä¸­æ–‡å­—ä½“"""
    try:
        import urllib.request
        import zipfile
        import tempfile
        
        # ä»GitHubä¸‹è½½å¼€æºä¸­æ–‡å­—ä½“
        font_url = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/NotoSansSC-Regular.otf"
        font_path = os.path.join(tempfile.gettempdir(), "NotoSansSC-Regular.otf")
        
        # ä¸‹è½½å­—ä½“æ–‡ä»¶
        urllib.request.urlretrieve(font_url, font_path)
        
        # æ·»åŠ åˆ°matplotlibå­—ä½“ç®¡ç†å™¨
        import matplotlib.font_manager as fm
        fm.fontManager.addfont(font_path)
        
        # è®¾ç½®å­—ä½“
        font_name = fm.FontProperties(fname=font_path).get_name()
        plt.rcParams['font.sans-serif'] = [font_name]
        plt.rcParams['axes.unicode_minus'] = False
        
        print(f"âœ“ å·²ä¸‹è½½å¹¶ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font_name}")
        return True
        
    except Exception as e:
        print(f"ä¸‹è½½å­—ä½“å¤±è´¥: {e}")
        print("âš  å°†ä½¿ç”¨é»˜è®¤å­—ä½“ï¼Œä¸­æ–‡å¯èƒ½æ˜¾ç¤ºä¸ºæ–¹æ¡†")
        return False

# åœ¨ç±»å®šä¹‰å‰è®¾ç½®å­—ä½“
setup_chinese_font()

class ChunkingEvaluator:
    """åˆ‡åˆ†æ•ˆæœè¯„ä¼°å™¨"""

    def __init__(self, embedding_model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(embedding_model_name)

    def compute_coherence_score(self, chunks: List[Chunk]) -> float:
        """
        è®¡ç®—è¯­ä¹‰è¿è´¯æ€§åˆ†æ•°
        ç›¸é‚»å—ä¹‹é—´çš„ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œè¿è´¯æ€§è¶Šå¥½
        """
        if len(chunks) < 2:
            return 1.0

        embeddings = self.model.encode([c.text for c in chunks])
        similarities = []

        for i in range(len(chunks) - 1):
            sim = cosine_similarity([embeddings[i]], [embeddings[i+1]])[0][0]
            similarities.append(sim)

        return np.mean(similarities)

    def compute_completeness_score(self, chunks: List[Chunk], original_text: str) -> float:
        """
        è®¡ç®—å®Œæ•´æ€§åˆ†æ•°
        æ–‡æœ¬è¦†ç›–ç‡
        """
        total_covered = sum(len(c.text) for c in chunks)
        overlap = sum(max(0, len(c.text) - 200) for c in chunks[:-1])  # ä¼°ç®—é‡å 
        unique_covered = total_covered - overlap

        return min(unique_covered / len(original_text), 1.0)

    def evaluate_chunking_strategy(self, text: str, strategy_func, **kwargs) -> dict:
        """è¯„ä¼°æŸç§åˆ‡åˆ†ç­–ç•¥"""
        chunks = strategy_func(text, **kwargs)

        coherence = self.compute_coherence_score(chunks)
        completeness = self.compute_completeness_score(chunks, text)

        # è®¡ç®—å¹³å‡å—å¤§å°å’Œæ–¹å·®
        chunk_sizes = [len(c.text) for c in chunks]
        avg_size = np.mean(chunk_sizes)
        std_size = np.std(chunk_sizes)

        return {
            'strategy': strategy_func.__name__,
            'chunk_count': len(chunks),
            'coherence_score': coherence,
            'completeness_score': completeness,
            'avg_chunk_size': avg_size,
            'std_chunk_size': std_size,
            'chunks': chunks
        }

# ç»¼åˆè¯„ä¼°
if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open('../Stage_2/xyj.txt', 'r', encoding='utf-8') as f:
        test_text = f.read()[:3000]

    chunker = AdvancedChunker()
    evaluator = ChunkingEvaluator()

    strategies = [
        ('å›ºå®šé•¿åº¦', chunker.chunk_by_fixed_length, {'chunk_size': 200, 'overlap': 20}),
        ('å›ºå®šé•¿åº¦(å¤§)', chunker.chunk_by_fixed_length, {'chunk_size': 300, 'overlap': 30}),
        ('å¥å­çº§', chunker.chunk_by_sentences, {}),
        ('é€’å½’åˆ‡åˆ†', chunker.chunk_by_recursive, {'chunk_size': 300, 'overlap': 30})
    ]

    results = []
    print("=" * 80)
    print("åˆ‡åˆ†ç­–ç•¥ç»¼åˆè¯„ä¼°")
    print("=" * 80)

    for name, func, params in strategies:
        result = evaluator.evaluate_chunking_strategy(test_text, func, **params)
        results.append(result)

        print(f"\nã€{name}ã€‘")
        print(f"  å—æ•°é‡: {result['chunk_count']}")
        print(f"  è¯­ä¹‰è¿è´¯æ€§: {result['coherence_score']:.3f} (è¶Šé«˜è¶Šå¥½)")
        print(f"  å®Œæ•´æ€§: {result['completeness_score']:.3f}")
        print(f"  å¹³å‡å—å¤§å°: {result['avg_chunk_size']:.0f} Â± {result['std_chunk_size']:.0f}")

    # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))

    # ç­–ç•¥å¯¹æ¯”
    names = [r['strategy'].replace('chunk_by_', '').replace('_', ' ') for r in results]
    coherence_scores = [r['coherence_score'] for r in results]
    completeness_scores = [r['completeness_score'] for r in results]

    axes[0, 0].bar(names, coherence_scores)
    axes[0,0].set_title('è¯­ä¹‰è¿è´¯æ€§å¯¹æ¯”')
    axes[0,0].set_ylabel('è¿è´¯æ€§åˆ†æ•°')
    axes[0,0].tick_params(axis='x', rotation=45)

    axes[0,1].bar(names, completeness_scores)
    axes[0,1].set_title('æ–‡æœ¬å®Œæ•´æ€§å¯¹æ¯”')
    axes[0,1].set_ylabel('å®Œæ•´æ€§åˆ†æ•°')
    axes[0,1].tick_params(axis='x', rotation=45)

    chunk_counts = [r['chunk_count'] for r in results]
    axes[1,0].bar(names, chunk_counts)
    axes[1,0].set_title('ç”Ÿæˆå—æ•°é‡å¯¹æ¯”')
    axes[1,0].set_ylabel('å—æ•°é‡')

    avg_sizes = [r['avg_chunk_size'] for r in results]
    axes[1,1].bar(names, avg_sizes)
    axes[1,1].set_title('å¹³å‡å—å¤§å°å¯¹æ¯”')
    axes[1,1].set_ylabel('å­—ç¬¦æ•°')

    plt.tight_layout()
    plt.savefig('chunking_evaluation.png', dpi=150, bbox_inches='tight')
    print("\nâœ… è¯„ä¼°å›¾è¡¨å·²ä¿å­˜ä¸º chunking_evaluation.png")

    # æ¨èç­–ç•¥
    print("\n" + "=" * 80)
    print("ç­–ç•¥æ¨è")
    print("=" * 80)
    print("ğŸ“Œ é€šç”¨åœºæ™¯: é€’å½’åˆ‡åˆ†")
    print("   - å¹³è¡¡äº†è¯­ä¹‰è¿è´¯æ€§å’Œå®Œæ•´æ€§")
    print("   - è‡ªé€‚åº”ä¸åŒç±»å‹çš„æ–‡æ¡£ç»“æ„")
    print("ğŸ“Œ ç®€å•æ–‡æ¡£: å›ºå®šé•¿åº¦(300å­—ç¬¦, 30é‡å )")
    print("   - é€Ÿåº¦å¿«ï¼Œå¯é¢„æµ‹")
    print("   - é€‚åˆæ—¥å¿—ã€ä»£ç ç­‰ç»“æ„åŒ–å†…å®¹")
    print("ğŸ“Œ æ–‡å­¦/å­¦æœ¯æ–‡æœ¬: å¥å­çº§åˆ‡åˆ†")
    print("   - ä¿æŒå¥å­å®Œæ•´æ€§")
    print("   - é€‚åˆè¯—æ­Œã€è®ºæ–‡ç­‰éœ€è¦ä¿æŒè¯­ä¹‰å•å…ƒçš„å†…å®¹")
```
    ================================================================================
    åˆ‡åˆ†ç­–ç•¥ç»¼åˆè¯„ä¼°
    ================================================================================
    
    ã€å›ºå®šé•¿åº¦ã€‘
      å—æ•°é‡: 17
      è¯­ä¹‰è¿è´¯æ€§: 0.772 (è¶Šé«˜è¶Šå¥½)
      å®Œæ•´æ€§: 1.000
      å¹³å‡å—å¤§å°: 195 Â± 19
    
    ã€å›ºå®šé•¿åº¦(å¤§)ã€‘
      å—æ•°é‡: 11
      è¯­ä¹‰è¿è´¯æ€§: 0.773 (è¶Šé«˜è¶Šå¥½)
      å®Œæ•´æ€§: 0.767
      å¹³å‡å—å¤§å°: 300 Â± 0
    
    ã€å¥å­çº§ã€‘
      å—æ•°é‡: 11
      è¯­ä¹‰è¿è´¯æ€§: 0.774 (è¶Šé«˜è¶Šå¥½)
      å®Œæ•´æ€§: 0.723
      å¹³å‡å—å¤§å°: 273 Â± 37
    
    ã€é€’å½’åˆ‡åˆ†ã€‘
      å—æ•°é‡: 12
      è¯­ä¹‰è¿è´¯æ€§: 0.804 (è¶Šé«˜è¶Šå¥½)
      å®Œæ•´æ€§: 0.798
      å¹³å‡å—å¤§å°: 269 Â± 27
    
    âœ… è¯„ä¼°å›¾è¡¨å·²ä¿å­˜ä¸º chunking_evaluation.png
    
    ================================================================================
    ç­–ç•¥æ¨è
    ================================================================================
    ğŸ“Œ é€šç”¨åœºæ™¯: é€’å½’åˆ‡åˆ†
       - å¹³è¡¡äº†è¯­ä¹‰è¿è´¯æ€§å’Œå®Œæ•´æ€§
       - è‡ªé€‚åº”ä¸åŒç±»å‹çš„æ–‡æ¡£ç»“æ„
    ğŸ“Œ ç®€å•æ–‡æ¡£: å›ºå®šé•¿åº¦(300å­—ç¬¦, 30é‡å )
       - é€Ÿåº¦å¿«ï¼Œå¯é¢„æµ‹
       - é€‚åˆæ—¥å¿—ã€ä»£ç ç­‰ç»“æ„åŒ–å†…å®¹
    ğŸ“Œ æ–‡å­¦/å­¦æœ¯æ–‡æœ¬: å¥å­çº§åˆ‡åˆ†
       - ä¿æŒå¥å­å®Œæ•´æ€§
       - é€‚åˆè¯—æ­Œã€è®ºæ–‡ç­‰éœ€è¦ä¿æŒè¯­ä¹‰å•å…ƒçš„å†…å®¹
    
    
**æµ‹è¯•æ—¥æœŸ**: 2025-12-05

#### å›ºå®šé•¿åº¦åˆ‡åˆ†

```python
# é…ç½®å‚æ•°
chunk_size = 200
overlap = 20

# æµ‹è¯•ç»“æœ
å—æ•°é‡: 17
å¹³å‡Tokenæ•°: 195.0 Â± 19
è¯­ä¹‰è¿è´¯æ€§åˆ†æ•°: 0.772
```

**è§‚å¯Ÿ**:
- é‡å æ¯”ä¾‹20%æ—¶ï¼Œä¸Šä¸‹ä¸¢å¤±æƒ…å†µ: overlap ä¸º 40 æ—¶æ¯” 20 é«˜4.4%
- æœ€ä¼˜overlapå€¼: 50 -> 0.810

#### é€’å½’åˆ‡åˆ†

```python
# é…ç½®å‚æ•°
chunk_size = 300
overlap = 30
separators = ['\n\n', '\n', 'ã€‚', '']

# æµ‹è¯•ç»“æœ
å—æ•°é‡: 12
å¹³å‡Tokenæ•°: 269.0 Â± 27.0
è¯­ä¹‰è¿è´¯æ€§åˆ†æ•°: 0.804
```

**è§‚å¯Ÿ**:
- é€’å½’åˆ‡åˆ† vs å›ºå®šé•¿åº¦çš„è¯­ä¹‰è¿è´¯æ€§æå‡: 4.4%
- é€‚ç”¨åœºæ™¯: 

é€šç”¨åœºæ™¯: é€’å½’åˆ‡åˆ†
   - å¹³è¡¡äº†è¯­ä¹‰è¿è´¯æ€§å’Œå®Œæ•´æ€§
   - è‡ªé€‚åº”ä¸åŒç±»å‹çš„æ–‡æ¡£ç»“æ„

ç®€å•æ–‡æ¡£: å›ºå®šé•¿åº¦(300å­—ç¬¦, 30é‡å )
   - é€Ÿåº¦å¿«ï¼Œå¯é¢„æµ‹
   - é€‚åˆæ—¥å¿—ã€ä»£ç ç­‰ç»“æ„åŒ–å†…å®¹

æ–‡å­¦/å­¦æœ¯æ–‡æœ¬: å¥å­çº§åˆ‡åˆ†
   - ä¿æŒå¥å­å®Œæ•´æ€§
   - é€‚åˆè¯—æ­Œã€è®ºæ–‡ç­‰éœ€è¦ä¿æŒè¯­ä¹‰å•å…ƒçš„å†…å®¹

#### å¯¹æ¯”ç»“æœ

| ç­–ç•¥ | å—æ•°é‡ | è¯­ä¹‰è¿è´¯æ€§ | å®Œæ•´æ€§ | æ¨èåº¦ |
|------|--------|------------|--------|--------|
| å›ºå®šé•¿åº¦ | 17 | 0.772 | 1.000 | â­â­ |
| å¥å­çº§ | 11 | 0.774 | 0.723 | â­â­â­ |
| é€’å½’åˆ‡åˆ† | 12 | 0.804 | 0.798 | â­â­â­â­â­ |

### å®éªŒä¸€ç»ƒä¹ é¢˜

1. **é‡å çª—å£ä¼˜åŒ–å®éªŒ**ï¼šåˆ†åˆ«æµ‹è¯• overlap = 0, 10, 20, 30, 40, 50ï¼Œæ‰¾å‡ºæœ€ä½³é‡å æ¯”ä¾‹
2. **æ··åˆç­–ç•¥å®éªŒ**ï¼šå¯¹æ–‡æ¡£ä¸åŒéƒ¨åˆ†ä½¿ç”¨ä¸åŒåˆ‡åˆ†ç­–ç•¥ï¼ˆå¦‚æ ‡é¢˜ç”¨å¥å­çº§ï¼Œæ­£æ–‡ç”¨é€’å½’ï¼‰
3. **é¢†åŸŸç‰¹åŒ–å®éªŒ**ï¼šå¯¹æ³•å¾‹æ–‡ä¹¦ã€åŒ»å­¦è®ºæ–‡ç­‰ç‰¹å®šé¢†åŸŸæ–‡æœ¬ï¼Œè®¾è®¡ä¸“ç”¨åˆ‡åˆ†ç­–ç•¥

### å‘ç°ä¸æ€è€ƒ

**é—®é¢˜1**: å½“æ–‡æ¡£ç»“æ„ä¸è§„å¾‹æ—¶ï¼Œå“ªç§ç­–ç•¥æœ€é²æ£’ï¼Ÿ
- ç­”æ¡ˆ: é€’å½’åˆ‡åˆ†
- è¯æ®: é€šè¿‡å¤šçº§åˆ†éš”ç¬¦ï¼ˆæ®µè½â†’å¥å­â†’è¯è¯­ï¼‰è‡ªåŠ¨é€‚åº”ä¸è§„å¾‹ç»“æ„ï¼Œèƒ½åœ¨è‡ªç„¶è¾¹ç•Œå¤„åˆ‡åˆ†ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œæœ€é€‚åˆæ··åˆæ ¼å¼æ–‡æ¡£

**é—®é¢˜2**: Overlapæ¯”ä¾‹å¯¹å¬å›ç‡çš„å½±å“ï¼Ÿ
- æµ‹è¯•äº†overlap = 0, 10, 20, 30, 50
- æœ€ä½³å€¼: 50
- åŸå› åˆ†æ: é‡å  50 å­—ç¬¦ç¡®ä¿å…³é”®ä¿¡æ¯ä¸ä¼šåœ¨åˆ‡åˆ†å¤„ä¸¢å¤±,å‘é‡æ£€ç´¢æ—¶ï¼Œå³ä½¿éƒ¨åˆ†ä¿¡æ¯åœ¨è¾¹ç•Œå¤„ï¼Œä»æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡è¢«åŒ¹é…åˆ°

**é—®é¢˜3**: å¦‚ä½•é’ˆå¯¹ç‰¹å®šé¢†åŸŸä¼˜åŒ–åˆ‡åˆ†ï¼Ÿ
- æ³•å¾‹æ–‡æ¡£: æŒ‰æ¡æ¬¾/ç« èŠ‚åˆ‡åˆ†ï¼Œä¿æŒæ¡æ¬¾å®Œæ•´æ€§ï¼Œè¯†åˆ«"ç¬¬Xæ¡"ã€"ç¬¬Xæ¬¾"ç­‰æ³•å¾‹æ ‡è®°
- æŠ€æœ¯æ–‡æ¡£: ä»£ç å—ä¸æ–‡æœ¬åˆ†ç¦»ï¼ŒAPIæ–‡æ¡£æŒ‰æ–¹æ³•/ç±»åˆ‡åˆ†ï¼Œä¿ç•™ä»£ç ä¸Šä¸‹æ–‡
- è´¢åŠ¡æŠ¥è¡¨: è¡¨æ ¼ç»“æ„åŒ–åˆ‡åˆ†ï¼ŒæŒ‰ä¼šè®¡ç§‘ç›®/æ—¶é—´å‘¨æœŸåˆ’åˆ†ï¼Œä¿ç•™è¡¨å¤´ä¸æ•°æ®å…³è”

## å®éªŒäºŒï¼šæ£€ç´¢å‡†ç¡®åº¦ä¼˜åŒ– (Retrieval Optimization)

### å®éªŒç›®æ ‡

æŒæ¡æ··åˆæ£€ç´¢ã€é‡æ’åºç­‰å…ˆè¿›æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡æ£€ç´¢è´¨é‡ã€‚

### ç†è®ºåŸºç¡€

#### 1. ä¼ ç»Ÿæ£€ç´¢ vs è¯­ä¹‰æ£€ç´¢

| ç»´åº¦ | å…³é”®è¯æ£€ç´¢ (BM25) | è¯­ä¹‰æ£€ç´¢ (Dense) | æ··åˆæ£€ç´¢ |
|------|------------------|------------------|----------|
| åŸç† | TF-IDFç»Ÿè®¡ | å‘é‡ç›¸ä¼¼åº¦ | åŠ æƒèåˆ |
| ä¼˜ç‚¹ | ç²¾ç¡®åŒ¹é…ã€é€Ÿåº¦å¿« | è¯­ä¹‰ç†è§£ã€é²æ£’æ€§å¥½ | ç»¼åˆä¼˜åŠ¿ |
| ç¼ºç‚¹ | æ— æ³•ç†è§£è¯­ä¹‰ | å¯èƒ½è¿‡åº¦æ³›åŒ– | å®ç°å¤æ‚ |
| é€‚ç”¨ | ç²¾ç¡®æŸ¥è¯¢ã€ä¸“ä¸šæœ¯è¯­ | åŒä¹‰è¡¨è¾¾ã€è¯­ä¹‰ç†è§£ | å¤æ‚æŸ¥è¯¢ |

#### 2. é‡æ’åº (Reranking)

**é—®é¢˜**ï¼šTop-Kæ£€ç´¢å¯èƒ½åŒ…å«ä¸ç›¸å…³ç»“æœ
**è§£å†³**ï¼šä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹å¯¹åˆæ­¥ç»“æœé‡æ–°æ’åº

```
åˆå§‹æ£€ç´¢ (100ä¸ªç»“æœ) â†’ é‡æ’åºæ¨¡å‹ â†’ Top-10æœ€ç»ˆç»“æœ
```

### å®éªŒæ­¥éª¤

#### Step 1: å®ç°æ··åˆæ£€ç´¢


```python
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from typing import List, Tuple
import jieba

class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨ï¼šBM25 + è¯­ä¹‰æ£€ç´¢"""

    def __init__(self, semantic_weight=0.7, bm25_weight=0.3):
        self.semantic_weight = semantic_weight
        self.bm25_weight = bm25_weight
        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chunks = []
        self.chunk_embeddings = None
        self.bm25 = None
        self.tokenized_chunks = []

    def fit(self, chunks: List[Chunk]):
        """
        æ„å»ºç´¢å¼•
        """
        self.chunks = chunks

        # 1. è¯­ä¹‰ç´¢å¼•
        self.chunk_embeddings = self.semantic_model.encode([c.text for c in chunks])

        # 2. BM25ç´¢å¼•
        tokenized_texts = []
        for chunk in chunks:
            # ä¸­æ–‡åˆ†è¯
            tokens = list(jieba.cut(chunk.text))
            tokenized_texts.append(tokens)

        self.tokenized_chunks = tokenized_texts
        self.bm25 = BM25Okapi(tokenized_texts)

        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, k: int = 5) -> List[Tuple[float, Chunk]]:
        """
        æ··åˆæ£€ç´¢
        """
        # 1. è¯­ä¹‰æ£€ç´¢
        query_embedding = self.semantic_model.encode([query])[0]
        semantic_similarities = np.dot(self.chunk_embeddings, query_embedding)
        semantic_similarities = (semantic_similarities + 1) / 2  # å½’ä¸€åŒ–åˆ°[0,1]

        # 2. BM25æ£€ç´¢
        query_tokens = list(jieba.cut(query))
        bm25_scores = self.bm25.get_scores(query_tokens)
        bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)

        # 3. åŠ æƒèåˆ
        hybrid_scores = (
            self.semantic_weight * semantic_similarities +
            self.bm25_weight * bm25_scores
        )

        # 4. Top-K
        top_indices = np.argsort(hybrid_scores)[::-1][:k]

        results = []
        for idx in top_indices:
            score = hybrid_scores[idx]
            results.append((score, self.chunks[idx]))

        return results

    def analyze_query(self, query: str, k: int = 5):
        """åˆ†ææ£€ç´¢è¿‡ç¨‹"""
        results = self.search(query, k)

        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("=" * 80)

        # è¯­ä¹‰æ£€ç´¢å¾—åˆ†
        query_embedding = self.semantic_model.encode([query])[0]
        semantic_similarities = np.dot(self.chunk_embeddings, query_embedding)
        semantic_similarities = (semantic_similarities + 1) / 2

        # BM25æ£€ç´¢å¾—åˆ†
        query_tokens = list(jieba.cut(query))
        bm25_scores = self.bm25.get_scores(query_tokens)
        bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)

        print(f"{'æ’å':<4} {'æ··åˆå¾—åˆ†':<10} {'è¯­ä¹‰å¾—åˆ†':<10} {'BM25å¾—åˆ†':<10} {'æ–‡æœ¬é¢„è§ˆ'}")
        print("-" * 80)

        for i, (score, chunk) in enumerate(results, 1):
            semantic_score = semantic_similarities[chunk.chunk_id]
            bm25_score = bm25_scores[chunk.chunk_id]
            preview = chunk.text[:50] + "..." if len(chunk.text) > 50 else chunk.text
            print(f"{i:<4} {score:<10.3f} {semantic_score:<10.3f} {bm25_score:<10.3f} {preview}")

        return results

# æµ‹è¯•æ··åˆæ£€ç´¢
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    with open('../Stage_2/anthropic.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    # åˆ‡åˆ†
    chunker = AdvancedChunker()
    chunks = chunker.chunk_by_recursive(text, 300, 30)

    # æ„å»ºæ··åˆæ£€ç´¢å™¨
    retriever = HybridRetriever(semantic_weight=0.7, bm25_weight=0.3)
    retriever.fit(chunks)

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Anthropicå…¬å¸çš„å‘å±•å†ç¨‹"
    ]

    for query in test_queries:
        retriever.analyze_query(query, k=5)
        print()
```

    âœ… ç´¢å¼•æ„å»ºå®Œæˆ: 12 ä¸ªæ–‡æ¡£å—
    
    ğŸ” æŸ¥è¯¢: Anthropicå…¬å¸çš„å‘å±•å†ç¨‹
    ================================================================================
    æ’å   æ··åˆå¾—åˆ†       è¯­ä¹‰å¾—åˆ†       BM25å¾—åˆ†     æ–‡æœ¬é¢„è§ˆ
    --------------------------------------------------------------------------------
    1    0.820      0.750      0.983      eçš„åˆ›ä½œï¼Œå¹¶å°†AIç”Ÿæˆçš„å†…å®¹æ— ç¼åœ°é›†æˆåˆ°ä»–ä»¬çš„é¡¹ç›®å’Œå·¥ä½œæµä¸­2024å¹´3æœˆ6æ—¥ï¼Œäºšé©¬é€Šå®£å¸ƒAnthr...
    2    0.802      0.717      1.000      Anthropicæ˜¯ç”±OpenAIå‰é«˜å±‚ä¸¹å¦®æ‹‰Â·é˜¿è«è¿ªï¼ˆDaniela Amodeiï¼‰å’Œè¾¾é‡Œå¥¥Â·é˜¿è«...
    3    0.796      0.783      0.826       Forumï¼‰ï¼Œè‡´åŠ›äºç¡®ä¿å®‰å…¨ã€è´Ÿè´£ä»»åœ°å¼€å‘å‰æ²¿äººå·¥æ™ºèƒ½æ¨¡å‹2023å¹´8æœˆ13æ—¥ï¼ŒéŸ©å›½æœ€å¤§ç”µä¿¡è¿è¥å•†...
    4    0.757      0.675      0.950      äººå·¦å³ï¼Œå¤§éƒ¨åˆ†æˆå‘˜æ›¾ç»å‚ä¸è¿‡GPT-2ã€GPT-3æ¨¡å‹çš„ç ”å‘åŒå¹´3æœˆ15æ—¥ï¼ŒAnthropicæ¨å‡ºèŠ...
    5    0.744      0.725      0.791      opicå‘å¸ƒäº†Claude 2.1ï¼Œæ‹¥æœ‰200Kçš„ä¸Šä¸‹æ–‡çª—å£åŒæœˆï¼Œæœ‰çŸ¥æƒ…äººå£«è¡¨ç¤ºï¼ŒAnthropic...
    
    

### æ··åˆæ£€ç´¢å®éªŒ

**æµ‹è¯•æ—¥æœŸ**: 2025-12-05

#### æƒé‡è°ƒä¼˜å®éªŒ

```python
# æµ‹è¯•é…ç½®
semantic_weight = 0.7
bm25_weight = 0.3

# æ£€ç´¢ç»“æœåˆ†æ
æŸ¥è¯¢: "Anthropicå…¬å¸å‘å±•å†ç¨‹"
Top-5ç»“æœ:
1. è¯­ä¹‰æ£€ç´¢å¾—åˆ†: 0.750, BM25å¾—åˆ†: 0.983
2. è¯­ä¹‰æ£€ç´¢å¾—åˆ†: 0.717, BM25å¾—åˆ†: 1.000
3. è¯­ä¹‰æ£€ç´¢å¾—åˆ†: 0.783, BM25å¾—åˆ†: 0.826
4. è¯­ä¹‰æ£€ç´¢å¾—åˆ†: 0.675, BM25å¾—åˆ†: 0.950
5. è¯­ä¹‰æ£€ç´¢å¾—åˆ†: 0.725, BM25å¾—åˆ†: 0.791
```

#### ä¸åŒæƒé‡é…ç½®å¯¹æ¯”

- çº¯è¯­ä¹‰ï¼š(1.0, 0.0)
- çº¯BM25ï¼š(0.0, 1.0)
- æ··åˆæ¯”ä¾‹ï¼š(0.7, 0.3)ã€(0.5, 0.5)ã€(0.3, 0.7)

| æ¯”ä¾‹ | æ··åˆå¾—åˆ† | è¯­ä¹‰å¾—åˆ† | BM25å¾—åˆ† | å¤‡æ³¨ |
|-----------------|-------------|----------|------|------|
| 1.0, 0.0 | 0.783 | 0.783 | 0.826 |  |
| 0.0, 1.0 | 1.000 | 0.717 | 1.000 |  |
| 0.7, 0.3 | 0.670 | 0.750 | 0.983 |  |
| 0.5, 0.5 | 0.867 | 0.750 | 0.983 |  |
| 0.3, 0.7 | 0.915 | 0.717 | 1.000 |  |

**æœ€ä½³é…ç½®**: 
- Alpha = 1.0 (çº¯è¯­ä¹‰)ï¼Œ
åœºæ™¯ï¼šæ¨èç³»ç»Ÿã€FAQé—®ç­”ï¼ˆç”¨æˆ·æè¿°é—®é¢˜ï¼Œå¯»æ‰¾æ ‡å‡†ç­”æ¡ˆï¼‰ã€è·¨è¯­è¨€æœç´¢ã€‚
ä¾‹å­ï¼š"å¦‚ä½•è°ƒæ•´å¿ƒæƒ…ï¼Ÿ" -> åŒ¹é…å¿ƒç†å¥åº·æ–‡æ¡£ã€‚
- Alpha = 0.0 (çº¯ BM25)ï¼Œ
åœºæ™¯ï¼šä»£ç æœç´¢ã€SKU/é›¶ä»¶ç¼–å·æœç´¢ã€æ³•å¾‹æ–‡ä¹¦ä¸­çš„ç‰¹å®šæ¡æ¬¾æœç´¢ã€‚
ä¾‹å­ï¼š"func get_user_id()" -> ç²¾ç¡®åŒ¹é…ä»£ç åº“ã€‚
- Alpha = 0.5 (å¹³è¡¡ï¼Œå¸¸ç”¨é»˜è®¤å€¼)ï¼Œ
åœºæ™¯ï¼šé€šç”¨çŸ¥è¯†åº“ã€RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿã€‚
åŸç†ï¼šæ—¢æƒ³è¦åŒä¹‰è¯æ‰©å±•ï¼Œåˆæƒ³ç¡®ä¿å…³é”®è¯å°½å¯èƒ½å‡ºç°ã€‚
- åå‘ BM25 (e.g., Semantic 0.3, BM25 0.7)ï¼Œ
åœºæ™¯ï¼šç”µå•†æœç´¢ã€‚ç”¨æˆ·æœ "è€å…‹ çº¢è‰² è·‘é‹"ï¼Œé€šå¸¸å¸Œæœ›è¿™ä¸‰ä¸ªè¯éƒ½ä¸¥æ ¼åŒ¹é…ï¼Œè€Œä¸æ˜¯æ¨è "é˜¿è¿ªè¾¾æ–¯ çº¢è‰² è·‘é‹"ï¼ˆè™½ç„¶è¯­ä¹‰ç›¸è¿‘ï¼Œä½†åœ¨ç”µå•†åœºæ™¯æ˜¯é”™è¯¯çš„ï¼‰ã€‚

#### Step 2: å®ç°é‡æ’åºæœºåˆ¶


```python
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import List, Tuple

class RerankRetriever:
    """å¸¦é‡æ’åºçš„æ£€ç´¢å™¨"""

    def __init__(self, cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-6-v2'):
        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.rerank_model = CrossEncoder(cross_encoder_model)
        self.chunks = []
        self.chunk_embeddings = None

    def fit(self, chunks: List[Chunk]):
        """æ„å»ºç´¢å¼•"""
        self.chunks = chunks
        self.chunk_embeddings = self.semantic_model.encode([c.text for c in chunks])
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, initial_k: int = 50, final_k: int = 5) -> List[Tuple[float, Chunk]]:
        """
        ä¸¤é˜¶æ®µæ£€ç´¢ï¼šåˆæ­¥æ£€ç´¢ + é‡æ’åº
        """
        # é˜¶æ®µ1: åˆæ­¥è¯­ä¹‰æ£€ç´¢
        query_embedding = self.semantic_model.encode([query])[0]
        similarities = np.dot(self.chunk_embeddings, query_embedding)
        top_indices = np.argsort(similarities)[::-1][:initial_k]

        # å‡†å¤‡é‡æ’åºçš„query-document pairs
        candidate_pairs = [(query, self.chunks[idx].text) for idx in top_indices]

        # é˜¶æ®µ2: ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹é‡æ’åº
        rerank_scores = self.rerank_model.predict(candidate_pairs)

        # è·å–æœ€ç»ˆTop-K
        reranked_indices = np.argsort(rerank_scores)[::-1][:final_k]

        results = []
        for idx in reranked_indices:
            original_idx = top_indices[idx]
            score = rerank_scores[idx]
            results.append((score, self.chunks[original_idx]))

        return results

    def search_with_ablation(self, query: str):
        """å¯¹æ¯”æœ‰æ— é‡æ’åºçš„æ•ˆæœ"""
        # æ— é‡æ’åº
        query_embedding = self.semantic_model.encode([query])[0]
        similarities = np.dot(self.chunk_embeddings, query_embedding)
        initial_top5 = np.argsort(similarities)[::-1][:5]

        # æœ‰é‡æ’åº
        results = self.search(query, initial_k=20, final_k=5)

        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("=" * 80)
        print("ã€ä»…è¯­ä¹‰æ£€ç´¢ Top-5ã€‘")
        for i, idx in enumerate(initial_top5, 1):
            print(f"{i}. {self.chunks[idx].text[:80]}...")
            print(f"   ç›¸ä¼¼åº¦: {similarities[idx]:.3f}")

        print("\nã€æ··åˆæ£€ç´¢ + é‡æ’åº Top-5ã€‘")
        for i, (score, chunk) in enumerate(results, 1):
            print(f"{i}. {chunk.text[:80]}...")
            print(f"   é‡æ’åºå¾—åˆ†: {score:.3f}")

        return results

# æµ‹è¯•é‡æ’åº
if __name__ == "__main__":
    # åŠ è½½ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡æœ¬ï¼ˆæ›´å¤§çš„æ•°æ®é›†ï¼‰
    with open('../Stage_2/hlm.txt', 'r', encoding='utf-8') as f:
        text = f.read()[:50000]  # å–å‰5ä¸‡å­—ç¬¦æµ‹è¯•

    chunker = AdvancedChunker()
    chunks = chunker.chunk_by_recursive(text, 300, 30)

    retriever = RerankRetriever()
    retriever.fit(chunks)

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "è´¾å®ç‰å’Œæ—é»›ç‰çš„å…³ç³»",
        "è–›å®é’—çš„æ€§æ ¼ç‰¹ç‚¹",
        "çº¢æ¥¼æ¢¦çš„ä¸»é¢˜æ€æƒ³"
    ]

    for query in test_queries:
        retriever.search_with_ablation(query)
        print("\n" + "=" * 80 + "\n")
```

    âœ… ç´¢å¼•æ„å»ºå®Œæˆ: 195 ä¸ªæ–‡æ¡£å—
    
    ğŸ” æŸ¥è¯¢: è´¾å®ç‰å’Œæ—é»›ç‰çš„å…³ç³»
    ================================================================================
    ã€ä»…è¯­ä¹‰æ£€ç´¢ Top-5ã€‘
    1. å¦æ¢äº†å››ä¸ªçœ‰ç›®ç§€æ´åä¸ƒå…«å²çš„å°å®ä¸Šæ¥æŠ¬ç€è½¿å­ï¼Œä¼—å©†å­æ­¥ä¸‹è·Ÿéšè‡³ä¸€å‚èŠ±é—¨å‰è½ä¸‹ï¼Œä¼—å°å®ä¿±è‚ƒç„¶é€€å‡ºï¼Œä¼—å©†å­ä¸Šå‰æ‰“èµ·è½¿å¸˜ï¼Œæ‰¶é»›ç‰ä¸‹äº†è½¿é»›ç‰æ‰¶ç€å©†å­çš„æ‰‹ï¼Œè¿›äº†å‚èŠ±é—¨ä¸¤è¾¹...
       ç›¸ä¼¼åº¦: 0.491
    2. ......
    
    ã€æ··åˆæ£€ç´¢ + é‡æ’åº Top-5ã€‘
    1. ï¼Œæ¸¸è§ˆå¤©ä¸‹èƒœè¿¹é‚£æ—¥å¶åˆæ¸¸è‡³ç»´æ‰¬åœ°æ–¹ï¼Œé—»å¾—ä»Šå¹´ç›æ”¿ç‚¹çš„æ˜¯æ—å¦‚æµ·è¿™æ—å¦‚æµ·ï¼Œå§“æ—ï¼Œåæµ·ï¼Œè¡¨å­—å¦‚æµ·ï¼Œä¹ƒæ˜¯å‰ç§‘çš„æ¢èŠ±ï¼Œä»Šå·²å‡å…°å°å¯ºå¤§å¤«æœ¬è´¯å§‘è‹äººæ°ï¼Œä»Šé’¦ç‚¹ä¸ºå·¡ç›å¾¡å²ï¼Œåˆ°ä»»...
       é‡æ’åºå¾—åˆ†: 8.531
    2. ......
    
    ================================================================================
    
   çœç•¥
    
    ================================================================================
    
    
    ğŸ” æŸ¥è¯¢: çº¢æ¥¼æ¢¦çš„ä¸»é¢˜æ€æƒ³
    ================================================================================
    ã€ä»…è¯­ä¹‰æ£€ç´¢ Top-5ã€‘
    1. ç‰æºæ‰‹åŒè¡Œå°¤æ°ç­‰é€è‡³å¤§å…å‰ï¼Œè§ç¯ç«è¾‰ç…Œï¼Œä¼—å°å®ä»¬éƒ½åœ¨ä¸¹å¢€ä¾ç«‹é‚£ç„¦å¤§åˆæƒè´¾çä¸åœ¨å®¶ï¼Œå› è¶ç€é…’å…´ï¼Œå…ˆéª‚å¤§æ€»ç®¡èµ–äºŒï¼Œè¯´ä»–â€œä¸å…¬é“ï¼æ¬ºè½¯æ€•ç¡¬ï¼æœ‰å¥½å·®ä½¿ï¼Œæ´¾äº†åˆ«äººï¼›è¿™æ ·é»‘...
       ç›¸ä¼¼åº¦: 0.501
    2. ......
    
    ã€æ··åˆæ£€ç´¢ + é‡æ’åº Top-5ã€‘
    1. ä¸»è¦äººç‰©çš„æ„Ÿæƒ…çº è‘›ï¼Œæå†™äº†å¤§è§‚å›­å†…å¤–ä¸€ç³»åˆ—é’å¹´ç”·å¥³çš„çˆ±æƒ…æ•…äº‹åŒæ—¶ï¼Œé€šè¿‡å¯¹è¿™äº›çˆ±æƒ…æ‚²å‰§äº§ç”Ÿçš„ç¤¾ä¼šç¯å¢ƒæç»˜ï¼Œç‰µæ¶‰åˆ°å°å»ºç¤¾ä¼šæ”¿æ²»æ³•å¾‹ã€å®—æ³•ã€å¦‡å¥³ã€é“å¾·ã€å©šå§»ç­‰æ–¹é¢çš„é—®...
       é‡æ’åºå¾—åˆ†: 8.598
    2. ......
    
    ================================================================================
    
    

#### Step 3: æ£€ç´¢è´¨é‡è¯„ä¼°


```python
import numpy as np
from collections import defaultdict
from typing import List, Dict, Set
import matplotlib.pyplot as plt
from typing import List, Tuple
import numpy as np
from sentence_transformers import SentenceTransformer

class Chunk:
    def __init__(self, chunk_id: int, text: str):
        self.chunk_id = chunk_id
        self.text = text
    
    def __repr__(self):
        return f"Chunk(id={self.chunk_id}, text={self.text[:30]}...)"

class SimpleSemanticRetriever:
    """
    çº¯è¯­ä¹‰æ£€ç´¢å™¨
    åŸºäº SentenceTransformer ç”Ÿæˆå‘é‡å¹¶è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    """

    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹
        self.semantic_model = SentenceTransformer(model_name)
        self.chunks: List[Chunk] = []
        self.chunk_embeddings = None

    def fit(self, chunks: List[Chunk]):
        """
        æ„å»ºç´¢å¼•ï¼šå°†æ‰€æœ‰æ–‡æ¡£å—è½¬æ¢ä¸ºå‘é‡
        """
        self.chunks = chunks
        if not chunks:
            print("âš ï¸ è­¦å‘Š: æ–‡æ¡£å—åˆ—è¡¨ä¸ºç©º")
            return

        print("ğŸ”„ æ­£åœ¨ç”Ÿæˆè¯­ä¹‰å‘é‡ç´¢å¼•...")
        # encode è¿”å› numpy arrayï¼Œé»˜è®¤ normalize_embeddings=False
        # å¦‚æœéœ€è¦ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå»ºè®® normalize_embeddings=True æˆ–è€…åœ¨ search æ—¶å½’ä¸€åŒ–
        # è¿™é‡Œä½¿ç”¨ all-MiniLM-L6-v2ï¼Œå®ƒé€šå¸¸é…åˆä½™å¼¦ç›¸ä¼¼åº¦ä½¿ç”¨
        self.chunk_embeddings = self.semantic_model.encode(
            [c.text for c in chunks], 
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True # å½’ä¸€åŒ–åï¼Œç‚¹ç§¯ç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦
        )
        print(f"âœ… è¯­ä¹‰ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, k: int = 5) -> List[Tuple[float, Chunk]]:
        """
        æ£€ç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£å—
        è¿”å›: List[(score, Chunk)]
        """
        if not self.chunks:
            return []

        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ (åŒæ ·è¿›è¡Œå½’ä¸€åŒ–)
        query_embedding = self.semantic_model.encode(
            [query], 
            convert_to_numpy=True, 
            normalize_embeddings=True
        )[0]

        # 2. è®¡ç®—ç›¸ä¼¼åº¦ (å½’ä¸€åŒ–å‘é‡çš„ç‚¹ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦)
        # scores shape: (num_chunks,)
        scores = np.dot(self.chunk_embeddings, query_embedding)

        # 3. è·å– Top-K ç´¢å¼•
        # np.argsort ä»å°åˆ°å¤§æ’åºï¼Œ[::-1] åè½¬ä¸ºä»å¤§åˆ°å°
        top_k_indices = np.argsort(scores)[::-1][:k]

        # 4. ç»„è£…ç»“æœ
        results = []
        for idx in top_k_indices:
            results.append((float(scores[idx]), self.chunks[idx]))

        return results

class RetrievalEvaluator:
    """æ£€ç´¢è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        self.relevance_judgments = {}  # æŸ¥è¯¢ -> ç›¸å…³æ–‡æ¡£IDé›†åˆ
        self.retrieval_results = defaultdict(list)

    def add_relevance_judgment(self, query: str, relevant_chunk_ids: Set[int]):
        """æ·»åŠ äººå·¥ç›¸å…³æ€§æ ‡æ³¨"""
        self.relevance_judgments[query] = relevant_chunk_ids

    def evaluate_retrieval(self, query: str, retrieved_chunks: List[Chunk], k: int = 10) -> Dict:
        """è¯„ä¼°æ£€ç´¢æ•ˆæœ"""
        retrieved_ids = {chunk.chunk_id for chunk in retrieved_chunks[:k]}
        relevant_ids = self.relevance_judgments.get(query, set())

        # Precision@k: æ£€ç´¢ç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        if k == 0:
            precision_k = 0
        else:
            precision_k = len(retrieved_ids & relevant_ids) / k

        # Recall@k: æ£€ç´¢å‡ºçš„ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        if len(relevant_ids) == 0:
            recall_k = 1.0
        else:
            recall_k = len(retrieved_ids & relevant_ids) / len(relevant_ids)

        # F1@k
        if precision_k + recall_k == 0:
            f1_k = 0
        else:
            f1_k = 2 * precision_k * recall_k / (precision_k + recall_k)

        # Average Precision (AP)
        ap = self._compute_ap(retrieved_chunks, relevant_ids, k)

        return {
            'precision@k': precision_k,
            'recall@k': recall_k,
            'f1@k': f1_k,
            'ap': ap
        }

    def _compute_ap(self, retrieved_chunks: List[Chunk], relevant_ids: Set[int], k: int) -> float:
        """è®¡ç®—Average Precision"""
        if not relevant_ids or k == 0:
            return 0.0

        precision_sum = 0.0
        relevant_retrieved = 0

        for i, chunk in enumerate(retrieved_chunks[:k]):
            if chunk.chunk_id in relevant_ids:
                relevant_retrieved += 1
                precision_at_i = relevant_retrieved / (i + 1)
                precision_sum += precision_at_i

        return precision_sum / len(relevant_ids)

    def compare_strategies(self, queries: List[str], retrievers: Dict[str, callable]) -> Dict:
        """å¯¹æ¯”ä¸åŒæ£€ç´¢ç­–ç•¥"""
        results = defaultdict(dict)

        for query in queries:
            print(f"\nè¯„ä¼°æŸ¥è¯¢: {query}")
            print("-" * 60)

            for strategy_name, retriever_func in retrievers.items():
                retrieved = retriever_func(query)
                metrics = self.evaluate_retrieval(query, retrieved, k=5)
                results[strategy_name][query] = metrics

                print(f"{strategy_name}:")
                print(f"  Precision@5: {metrics['precision@k']:.3f}")
                print(f"  Recall@5: {metrics['recall@k']:.3f}")
                print(f"  F1@5: {metrics['f1@k']:.3f}")

        return results

    def plot_comparison(self, results: Dict):
        """å¯è§†åŒ–å¯¹æ¯”ç»“æœ"""
        strategies = list(results.keys())
        metrics = ['precision@k', 'recall@k', 'f1@k']

        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        for i, metric in enumerate(metrics):
            scores = []
            for strategy in strategies:
                # è®¡ç®—æ‰€æœ‰æŸ¥è¯¢çš„å¹³å‡åˆ†æ•°
                values = [results[strategy][q][metric] for q in results[strategy].keys()]
                scores.append(np.mean(values))

            axes[i].bar(strategies, scores)
            axes[i].set_title(f'{metric.upper()} å¯¹æ¯”')
            axes[i].set_ylabel('åˆ†æ•°')
            axes[i].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig('retrieval_comparison.png', dpi=150, bbox_inches='tight')
        print("âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ä¸º retrieval_comparison.png")

# è¯„ä¼°å®ä¾‹
if __name__ == "__main__":
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    with open('../Stage_2/anthropic.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    chunker = AdvancedChunker()
    chunks = chunker.chunk_by_recursive(text, 300, 30)

    # æ„å»ºä¸åŒæ£€ç´¢å™¨
    semantic_retriever = SimpleSemanticRetriever()
    semantic_retriever.fit(chunks)

    hybrid_retriever = HybridRetriever()
    hybrid_retriever.fit(chunks)

    # è¯„ä¼°å™¨
    evaluator = RetrievalEvaluator()

    # å‡†å¤‡æŸ¥è¯¢å’Œæ ‡æ³¨ï¼ˆè¿™é‡Œç”¨å…³é”®è¯åŒ¹é…æ¨¡æ‹Ÿäººå·¥æ ‡æ³¨ï¼‰
    test_queries_config = {
    "Anthropicå…¬å¸æˆç«‹æ—¶é—´": ["anthropic", "æˆç«‹", "æ—¶é—´"],
    "Claudeæ¨¡å‹ç‰¹ç‚¹": ["claude", "ç‰¹ç‚¹", "æ¨¡å‹"],
    "AIå®‰å…¨ç ”ç©¶": ["å®‰å…¨", "safety", "research"],
    "èèµ„è½®æ¬¡": ["èèµ„", "funding", "investment"],
    "åˆ›å§‹äººä¿¡æ¯": ["åˆ›å§‹äºº", "founder", "ceo", "dario"]
    }   

    # ä¸ºæ¯ä¸ªæŸ¥è¯¢æ ‡æ³¨ç›¸å…³æ–‡æ¡£ï¼ˆç®€åŒ–ç‰ˆï¼šåŒ…å«å…³é”®è¯çš„å³ä¸ºç›¸å…³ï¼‰
    queries = list(test_queries_config.keys()) # æå–æŸ¥è¯¢æ–‡æœ¬
    
    for query_text, keywords in test_queries_config.items():
        # åªè¦æ–‡æ¡£åŒ…å«åˆ—è¡¨ä¸­çš„ *ä»»æ„ä¸€ä¸ª* å…³é”®è¯ï¼Œå°±ç®—ç›¸å…³ï¼ˆæˆ–è€…ä½ å¯ä»¥æ”¹æˆ *æ‰€æœ‰* å…³é”®è¯ï¼‰
        relevant_ids = {
            chunk.chunk_id
            for chunk in chunks
            if any(kw.lower() in chunk.text.lower() for kw in keywords)
        }
    
        # å…³é”®ï¼šæ·»åŠ æ‰“å°è°ƒè¯•ï¼Œçœ‹çœ‹æ˜¯ä¸æ˜¯ç©ºçš„
        print(f"æŸ¥è¯¢: '{query_text}' -> å…³é”®è¯: {keywords} -> å‘½ä¸­ç›¸å…³æ–‡æ¡£æ•°: {len(relevant_ids)}")
    
        if len(relevant_ids) == 0:
            print(f"âš ï¸ è­¦å‘Š: æŸ¥è¯¢ '{query_text}' æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ–‡æ¡£ï¼è¯·æ£€æŸ¥å…³é”®è¯æˆ–æ–‡æ¡£å†…å®¹ã€‚")
        
        evaluator.add_relevance_judgment(query_text, relevant_ids)

    # å¯¹æ¯”ç­–ç•¥
    retrievers = {
        'è¯­ä¹‰æ£€ç´¢': lambda q: [x[1] for x in semantic_retriever.search(q, k=5)],
        'æ··åˆæ£€ç´¢': lambda q: [x[1] for x in hybrid_retriever.search(q, k=5)]
    }
    
    comparison_results = evaluator.compare_strategies(test_queries, retrievers)
    evaluator.plot_comparison(comparison_results)
```

## å®éªŒä¸‰ï¼šä¸Šä¸‹æ–‡çª—å£ä¸æˆæœ¬æ§åˆ¶ (Context Window Management)

### å®éªŒç›®æ ‡

è§£å†³é•¿æ–‡æ¡£å¤„ç†å’ŒAPIè°ƒç”¨æˆæœ¬æ§åˆ¶é—®é¢˜ï¼Œå®ç°é«˜æ•ˆçš„ç”Ÿäº§çº§RAGç³»ç»Ÿã€‚

### ç†è®ºåŸºç¡€

#### 1. ä¸Šä¸‹æ–‡çª—å£ç®¡ç†é—®é¢˜

**æŒ‘æˆ˜**:
- LLMä¸Šä¸‹æ–‡çª—å£æœ‰é™
- APIè°ƒç”¨æˆæœ¬éštokenæ•°çº¿æ€§å¢é•¿
- æ£€ç´¢å†…å®¹è¿‡å¤šä¼šç¨€é‡Šç›¸å…³ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**:
```
å¤šè½®æ£€ç´¢ â†’ åŠ¨æ€é€‰æ‹© â†’ ä¸Šä¸‹æ–‡å‹ç¼© â†’ æˆæœ¬æ§åˆ¶
```

#### 2. æˆæœ¬æ¨¡å‹

```
æ€»æˆæœ¬ = (Prompt Tokens + Context Tokens + Output Tokens) Ã— å•Tokenä»·æ ¼
```

### å®éªŒæ­¥éª¤

#### Step 1: å®ç°åŠ¨æ€ä¸Šä¸‹æ–‡é€‰æ‹©


```python
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Tuple
import math

@dataclass
class TokenCount:
    """Tokenè®¡æ•°å™¨"""
    prompt_base = 50  # åŸºç¡€prompt tokenæ•°
    output_estimate = 100  # é¢„ä¼°è¾“å‡ºtokenæ•°

    def count_chunks(self, chunks: List[Chunk]) -> int:
        """ä¼°ç®—chunkçš„tokenæ•°"""
        return sum(len(c.text) for c in chunks)

    def calculate_context_cost(
        self,
        chunks: List[Chunk],
        max_context_tokens: int = 8000,
        min_chunks: int = 1,
        max_chunks: int = 20
    ) -> Dict:
        """
        è®¡ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨æƒ…å†µ
        æ³¨æ„ï¼šè¾“å…¥çš„ chunks å¿…é¡»å·²ç»æŒ‰ä¼˜å…ˆçº§ï¼ˆåˆ†æ•°ï¼‰æ’å¥½åº
        """
        if not chunks:
            return {'selected_chunks': [], 'total_tokens': 0, 'chunk_count': 0}

        # ã€ä¿®å¤ç‚¹ã€‘ï¼šç§»é™¤ sorted() è°ƒç”¨
        # åŸå› ï¼šchunks æ˜¯ Chunk å¯¹è±¡åˆ—è¡¨ï¼Œæ²¡æœ‰ .get() æ–¹æ³•ï¼Œä¸”è°ƒç”¨æ–¹å·²ä¿è¯é¡ºåº
        sorted_chunks = chunks 

        # è´ªå¿ƒé€‰æ‹©ï¼šæŒ‰é¡ºåºæ·»åŠ ï¼Œç›´åˆ° Token è€—å°½
        selected = []
        total_tokens = self.prompt_base

        for chunk in sorted_chunks:
            # è¿™é‡Œç®€å•ç”¨å­—ç¬¦é•¿åº¦ä¼°ç®— Tokenï¼Œå®é™…ç”Ÿäº§ç¯å¢ƒå¯ç”¨ tiktoken
            chunk_tokens = len(chunk.text) 
            potential_total = total_tokens + chunk_tokens + self.output_estimate

            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            if potential_total <= max_context_tokens and len(selected) < max_chunks:
                selected.append(chunk)
                total_tokens += chunk_tokens
            
            # å¦‚æœå·²ç»æ»¡è¶³æœ€å° chunk æ•°ï¼Œä¸” token ç”¨é‡å·²è¾¾ 80%ï¼Œæå‰ç»“æŸ
            # (è¿™ä¸ªé€»è¾‘æ˜¯å¯é€‰çš„ï¼Œçœ‹æ˜¯å¦æƒ³å°½å¯èƒ½å¡«æ»¡çª—å£)
            if len(selected) >= min_chunks and total_tokens > max_context_tokens * 0.8:
                break

        return {
            'selected_chunks': selected,
            'total_tokens': total_tokens + self.output_estimate,
            'chunk_count': len(selected),
            'compression_ratio': len(chunks) / len(selected) if selected else 0
        }

class IntelligentContextSelector:
    """æ™ºèƒ½ä¸Šä¸‹æ–‡é€‰æ‹©å™¨"""

    def __init__(self, max_context_tokens=8000):
        self.max_context_tokens = max_context_tokens
        self.token_counter = TokenCount()

    def select_optimal_context(
        self,
        query: str,
        retrieval_results: List[Tuple[float, Chunk]],
        min_chunks: int = 2,
        max_chunks: int = 10
    ) -> Dict:
        """
        åŸºäºå¤šæ ·æ€§å’Œç›¸å…³æ€§çš„ä¸Šä¸‹æ–‡é€‰æ‹©
        """
        if not retrieval_results:
            return {'context': '', 'selected_chunks': [], 'reasoning': 'No results'}

        # å°†æ£€ç´¢ç»“æœè½¬æ¢ä¸ºdictæ ¼å¼ï¼ˆåŒ…å«åˆ†æ•°ï¼‰
        chunks_with_scores = [
            {'chunk': chunk, 'score': score}
            for score, chunk in retrieval_results
        ]

        # å¤šæ ·æ€§é€‰æ‹©ï¼šé¿å…é€‰æ‹©è¿‡äºç›¸ä¼¼çš„chunk
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer('all-MiniLM-L6-v2')

        embeddings = model.encode([c['chunk'].text for c in chunks_with_scores])
        similarity_matrix = np.dot(embeddings, embeddings.T)

        # åŸºäºå¤šæ ·æ€§çš„é€‰æ‹©
        selected_indices = []
        selected_embeddings = []

        # ä¼˜å…ˆé€‰æ‹©é«˜åˆ†chunk
        chunks_sorted = sorted(chunks_with_scores, key=lambda x: x['score'], reverse=True)

        for chunk_info in chunks_sorted:
            idx = chunks_with_scores.index(chunk_info)

            # æ£€æŸ¥ä¸å·²é€‰æ‹©chunkçš„ç›¸ä¼¼åº¦
            if not selected_embeddings:
                selected_indices.append(idx)
                selected_embeddings.append(embeddings[idx])
            else:
                # è®¡ç®—ä¸å·²é€‰æ‹©chunkçš„æœ€å¤§ç›¸ä¼¼åº¦
                max_sim = max(
                    np.dot(embeddings[idx], emb) / (
                        np.linalg.norm(embeddings[idx]) * np.linalg.norm(emb)
                    )
                    for emb in selected_embeddings
                )

                # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼ˆ0.8ï¼‰ï¼Œåˆ™é€‰æ‹©
                if max_sim < 0.8:
                    selected_indices.append(idx)
                    selected_embeddings.append(embeddings[idx])

            if len(selected_indices) >= max_chunks:
                break

        # é‡æ–°æ’åºï¼ˆæŒ‰åˆ†æ•°ï¼‰
        selected_indices = sorted(
            selected_indices,
            key=lambda i: chunks_with_scores[i]['score'],
            reverse=True
        )

        selected_chunks = [chunks_with_scores[i]['chunk'] for i in selected_indices]

        # è®¡ç®—tokenä½¿ç”¨æƒ…å†µ
        cost_info = self.token_counter.calculate_context_cost(
            selected_chunks,
            max_context_tokens=self.max_context_tokens,
            min_chunks=min_chunks
        )

        # æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡
        context_parts = []
        for i, chunk in enumerate(cost_info['selected_chunks'], 1):
            context_parts.append(f"[{i}] {chunk.text}")

        context = "\n\n".join(context_parts)

        return {
            'context': context,
            'selected_chunks': cost_info['selected_chunks'],
            'total_tokens': cost_info['total_tokens'],
            'chunk_count': cost_info['chunk_count'],
            'compression_ratio': cost_info['compression_ratio'],
            'diversity_score': len(selected_indices) / len(chunks_with_scores)
        }

# æµ‹è¯•ä¸Šä¸‹æ–‡é€‰æ‹©
if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•æ•°æ®
    with open('../Stage_2/anthropic.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    chunker = AdvancedChunker()
    chunks = chunker.chunk_by_recursive(text, 300, 30)

    retriever = SimpleSemanticRetriever()
    retriever.fit(chunks)

    selector = IntelligentContextSelector(max_context_tokens=4000)

    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Anthropicçš„èèµ„å’Œä¼°å€¼æ˜¯å¤šå°‘ï¼Ÿ",
        "Claude 3.5æœ‰ä»€ä¹ˆæ–°åŠŸèƒ½ï¼Ÿ",
        "AIå®‰å…¨æ–¹é¢ä»–ä»¬åšäº†ä»€ä¹ˆï¼Ÿ"
    ]

    for query in test_queries:
        print(f"\n{'='*80}")
        print(f"æŸ¥è¯¢: {query}")
        print('='*80)

        # æ£€ç´¢æ›´å¤šç»“æœç”¨äºé€‰æ‹©
        retrieval_results = retriever.search(query, k=20)

        # æ™ºèƒ½é€‰æ‹©ä¸Šä¸‹æ–‡
        context_info = selector.select_optimal_context(
            query,
            retrieval_results,
            min_chunks=2,
            max_chunks=5
        )

        print(f"\nğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:")
        print(f"  é€‰æ‹©chunkæ•°: {context_info['chunk_count']}")
        print(f"  æ€»Tokenæ•°: {context_info['total_tokens']}")
        print(f"  å‹ç¼©æ¯”: {context_info['compression_ratio']:.2f}x")
        print(f"  å¤šæ ·æ€§å¾—åˆ†: {context_info['diversity_score']:.3f}")

        print(f"\nğŸ“„ é€‰æ‹©çš„ä¸Šä¸‹æ–‡:")
        print(context_info['context'][:50] + "..." if len(context_info['context']) > 50 else context_info['context'])
```

    ğŸ”„ æ­£åœ¨ç”Ÿæˆè¯­ä¹‰å‘é‡ç´¢å¼•...
    âœ… è¯­ä¹‰ç´¢å¼•æ„å»ºå®Œæˆ: 2 ä¸ªæ–‡æ¡£å—
    
    ================================================================================
    æŸ¥è¯¢: Anthropicçš„èèµ„å’Œä¼°å€¼æ˜¯å¤šå°‘ï¼Ÿ
    ================================================================================
    
    ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡:
      é€‰æ‹©chunkæ•°: 1
      æ€»Tokenæ•°: 3722
      å‹ç¼©æ¯”: 2.00x
      å¤šæ ·æ€§å¾—åˆ†: 1.000
    
    ğŸ“„ é€‰æ‹©çš„ä¸Šä¸‹æ–‡:
    [1] Anthropic ä¸ Claudeï¼šäººå·¥æ™ºèƒ½å®‰å…¨ä¸å¤§æ¨¡å‹æ¼”è¿›æ·±åº¦æŠ¥å‘Š ç‰ˆæœ¬æ—¥æœŸï¼š2024å¹´...
    
    ================================================================================
    æŸ¥è¯¢: Claude 3.5æœ‰ä»€ä¹ˆæ–°åŠŸèƒ½ï¼Ÿ
    ================================================================================
    
    çœç•¥
    

#### Step 2: å®ç°æˆæœ¬è·Ÿè¸ªä¸é¢„ç®—æ§åˆ¶


```python
import json
from datetime import datetime
from typing import Dict, List
import matplotlib.pyplot as plt

class CostTracker:
    """APIè°ƒç”¨æˆæœ¬è·Ÿè¸ªå™¨"""

    # ä¸»æµLLMå®šä»·ï¼ˆæ¯1K tokensï¼‰
    PRICING = {
        'gpt-4': {'input': 0.03, 'output': 0.06},
        'gpt-3.5-turbo': {'input': 0.001, 'output': 0.002},
        'claude-3': {'input': 0.015, 'output': 0.075},
        'deepseek-chat': {'input': 0.00014, 'output': 0.00028},
        'deepseek-coder': {'input': 0.00014, 'output': 0.00028}
    }

    def __init__(self, budget_limit: float = 10.0):
        self.budget_limit = budget_limit  # ç¾å…ƒ
        self.total_spent = 0.0
        self.call_history = []
        self.daily_spending = defaultdict(float)

    def log_api_call(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        query: str
    ):
        """è®°å½•ä¸€æ¬¡APIè°ƒç”¨"""
        if model not in self.PRICING:
            print(f"âš ï¸ è­¦å‘Š: æœªçŸ¥æ¨¡å‹ {model}")
            return

        pricing = self.PRICING[model]
        cost = (
            input_tokens / 1000 * pricing['input'] +
            output_tokens / 1000 * pricing['output']
        )

        self.total_spent += cost

        call_info = {
            'timestamp': datetime.now().isoformat(),
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'cost': cost,
            'query': query[:100]  # æˆªæ–­ä¿å­˜
        }

        self.call_history.append(call_info)

        # æŒ‰æ—¥ç»Ÿè®¡
        date = datetime.now().strftime('%Y-%m-%d')
        self.daily_spending[date] += cost

        # æ£€æŸ¥é¢„ç®—
        if self.total_spent > self.budget_limit:
            print(f"âš ï¸ è­¦å‘Š: å·²è¶…å‡ºé¢„ç®—é™åˆ¶ ${self.budget_limit:.2f}")

        return cost

    def estimate_query_cost(
        self,
        query: str,
        context_chunks: List[Chunk],
        model: str = 'deepseek-chat'
    ) -> Dict:
        """ä¼°ç®—æŸ¥è¯¢æˆæœ¬"""
        if model not in self.PRICING:
            return {'error': f'æœªçŸ¥æ¨¡å‹: {model}'}

        # ä¼°ç®—tokenæ•°
        prompt_tokens = 50  # åŸºç¡€prompt
        query_tokens = len(query)
        context_tokens = sum(len(c.text) for c in context_chunks)
        output_tokens = 100  # é¢„ä¼°è¾“å‡º

        input_tokens = prompt_tokens + query_tokens + context_tokens

        pricing = self.PRICING[model]
        cost = (
            input_tokens / 1000 * pricing['input'] +
            output_tokens / 1000 * pricing['output']
        )

        return {
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'estimated_cost': cost,
            'cost_breakdown': {
                'prompt': prompt_tokens,
                'query': query_tokens,
                'context': context_tokens,
                'output': output_tokens
            }
        }

    def optimize_for_budget(
        self,
        query: str,
        retrieval_results: List[Tuple[float, Chunk]],
        target_cost: float,
        model: str = 'deepseek-chat'
    ) -> List[Chunk]:
        """åœ¨é¢„ç®—çº¦æŸä¸‹ä¼˜åŒ–æŸ¥è¯¢"""
        if model not in self.PRICING:
            return []

        pricing = self.PRICING[model]
        max_input_tokens = int(
            target_cost * 1000 / pricing['input'] - 100  # é¢„ç•™è¾“å‡ºç©ºé—´
        )

        # æŒ‰åˆ†æ•°æ’åº
        chunks_sorted = sorted(retrieval_results, key=lambda x: x[0], reverse=True)

        selected_chunks = []
        total_tokens = 50  # prompt + query

        for score, chunk in chunks_sorted:
            chunk_tokens = len(chunk.text)
            if total_tokens + chunk_tokens <= max_input_tokens:
                selected_chunks.append(chunk)
                total_tokens += chunk_tokens
            else:
                break

        return selected_chunks

    def generate_report(self) -> str:
        """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
        report = f"\n{'='*60}\n"
        report += f"ğŸ“Š APIè°ƒç”¨æˆæœ¬æŠ¥å‘Š\n"
        report += f"{'='*60}\n\n"

        report += f"ğŸ’° æ€»æ”¯å‡º: ${self.total_spent:.4f}\n"
        report += f"ğŸ’° é¢„ç®—é™åˆ¶: ${self.budget_limit:.2f}\n"
        report += f"ğŸ’° é¢„ç®—ä½¿ç”¨ç‡: {(self.total_spent/self.budget_limit)*100:.1f}%\n\n"

        report += f"ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: {len(self.call_history)}\n\n"

        # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
        model_usage = {}
        for call in self.call_history:
            model = call['model']
            if model not in model_usage:
                model_usage[model] = {'count': 0, 'cost': 0}
            model_usage[model]['count'] += 1
            model_usage[model]['cost'] += call['cost']

        report += f"ğŸ“ˆ æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡:\n"
        for model, stats in sorted(model_usage.items(), key=lambda x: x[1]['cost'], reverse=True):
            report += f"  {model}: {stats['count']}æ¬¡, ${stats['cost']:.4f}\n"

        report += f"\nğŸ“… æ¯æ—¥æ”¯å‡º:\n"
        for date, amount in sorted(self.daily_spending.items()):
            report += f"  {date}: ${amount:.4f}\n"

        return report

    def plot_spending(self, save_path: str = 'cost_analysis.png'):
        """ç»˜åˆ¶æ”¯å‡ºåˆ†æå›¾"""
        if not self.call_history:
            print("âš ï¸ æ²¡æœ‰è°ƒç”¨è®°å½•")
            return

        # ç´¯ç§¯æ”¯å‡º
        dates = [call['timestamp'][:10] for call in self.call_history]
        costs = [call['cost'] for call in self.call_history]
        cumulative_costs = np.cumsum(costs)

        fig, axes = plt.subplots(2, 2, figsize=(12, 8))

        # ç´¯ç§¯æ”¯å‡ºè¶‹åŠ¿
        axes[0,0].plot(range(len(cumulative_costs)), cumulative_costs)
        axes[0,0].set_title('ç´¯ç§¯æ”¯å‡ºè¶‹åŠ¿')
        axes[0,0].set_xlabel('è°ƒç”¨æ¬¡æ•°')
        axes[0,0].set_ylabel('ç´¯ç§¯æˆæœ¬ ($)')

        # æ¯æ¬¡è°ƒç”¨æˆæœ¬åˆ†å¸ƒ
        axes[0,1].hist(costs, bins=20, edgecolor='black')
        axes[0,1].set_title('å•æ¬¡è°ƒç”¨æˆæœ¬åˆ†å¸ƒ')
        axes[0,1].set_xlabel('æˆæœ¬ ($)')
        axes[0,1].set_ylabel('é¢‘æ¬¡')

        # æŒ‰æ¨¡å‹åˆ†ç»„çš„æˆæœ¬
        models = [call['model'] for call in self.call_history]
        unique_models = list(set(models))
        model_costs = [sum(call['cost'] for call in self.call_history if call['model'] == m) for m in unique_models]

        axes[1,0].bar(unique_models, model_costs)
        axes[1,0].set_title('å„æ¨¡å‹æˆæœ¬')
        axes[1,0].set_xlabel('æ¨¡å‹')
        axes[1,0].set_ylabel('æ€»æˆæœ¬ ($)')
        axes[1,0].tick_params(axis='x', rotation=45)

        # é¢„ç®—ä½¿ç”¨ç‡
        budget_used = (self.total_spent / self.budget_limit) * 100
        labels = ['å·²ä½¿ç”¨', 'å‰©ä½™']
        sizes = [budget_used, 100 - budget_used]
        axes[1,1].pie(sizes, labels=labels, autopct='%1.1f%%')
        axes[1,1].set_title('é¢„ç®—ä½¿ç”¨ç‡')

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… æˆæœ¬åˆ†æå›¾å·²ä¿å­˜ä¸º {save_path}")

# æµ‹è¯•æˆæœ¬æ§åˆ¶
if __name__ == "__main__":
    tracker = CostTracker(budget_limit=1.0)

    # æ¨¡æ‹ŸAPIè°ƒç”¨
    sample_calls = [
        ('deepseek-chat', 2000, 500, "Anthropicå…¬å¸ä»‹ç»"),
        ('gpt-3.5-turbo', 2000, 500, "Claudeæ¨¡å‹ç‰¹ç‚¹"),
        ('deepseek-coder', 2000, 500, "RAGç³»ç»ŸåŸç†"),
        ('gpt-4', 2000, 500, "AIå®‰å…¨ç ”ç©¶è¿›å±•"),
        ('claude-3', 2000, 500, "Anthropicèèµ„æƒ…å†µ")
    ]

    for model, input_tokens, output_tokens, query in sample_calls:
        cost = tracker.log_api_call(model, input_tokens, output_tokens, query)
        print(f"è°ƒç”¨ {model}: ${cost:.4f}")

    # æˆæœ¬ä¼°ç®—
    with open('../Stage_2/anthropic.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    chunker = AdvancedChunker()
    chunks = chunker.chunk_by_recursive(text, 300, 30)

    cost_info = tracker.estimate_query_cost("å…¬å¸å‘å±•å†ç¨‹", chunks[:3])
    print(f"\nğŸ’¡ æŸ¥è¯¢æˆæœ¬ä¼°ç®—: ${cost_info['estimated_cost']:.4f}")
    print(f"   Tokenåˆ†è§£: {cost_info['cost_breakdown']}")

    # ç”ŸæˆæŠ¥å‘Š
    print(tracker.generate_report())

    # ç»˜åˆ¶åˆ†æå›¾
    tracker.plot_spending()
```

    è°ƒç”¨ deepseek-chat: $0.0004
    è°ƒç”¨ gpt-3.5-turbo: $0.0030
    è°ƒç”¨ deepseek-coder: $0.0004
    è°ƒç”¨ gpt-4: $0.0900
    è°ƒç”¨ claude-3: $0.0675
    
    ğŸ’¡ æŸ¥è¯¢æˆæœ¬ä¼°ç®—: $0.0006
       Tokenåˆ†è§£: {'prompt': 50, 'query': 6, 'context': 4325, 'output': 100}
    
    ============================================================
    ğŸ“Š APIè°ƒç”¨æˆæœ¬æŠ¥å‘Š
    ============================================================
    
    ğŸ’° æ€»æ”¯å‡º: $0.1613
    ğŸ’° é¢„ç®—é™åˆ¶: $1.00
    ğŸ’° é¢„ç®—ä½¿ç”¨ç‡: 16.1%
    
    ğŸ“ æ€»è°ƒç”¨æ¬¡æ•°: 5
    
    ğŸ“ˆ æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡:
      gpt-4: 1æ¬¡, $0.0900
      claude-3: 1æ¬¡, $0.0675
      gpt-3.5-turbo: 1æ¬¡, $0.0030
      deepseek-chat: 1æ¬¡, $0.0004
      deepseek-coder: 1æ¬¡, $0.0004
    
    ğŸ“… æ¯æ—¥æ”¯å‡º:
      2025-12-08: $0.1613
    
    âœ… æˆæœ¬åˆ†æå›¾å·²ä¿å­˜ä¸º cost_analysis.png
    
---

## å®éªŒå››ï¼šå®æˆ˜é¡¹ç›® - æ™ºèƒ½æŠ•åæŠ¥å‘Šåˆ†æåŠ©æ‰‹

### é¡¹ç›®èƒŒæ™¯

**åœºæ™¯**ï¼šæŠ•èµ„æœºæ„éœ€è¦åˆ†æè¢«æŠ•ä¼ä¸šçš„è´¢æŠ¥å’ŒæŠ•åæŠ¥å‘Šï¼Œå›ç­”å¦‚"å¯¹æ¯”Aå…¬å¸å’ŒBå…¬å¸çš„è¥æ”¶å¢é•¿ç‡"ç­‰å¤æ‚é—®é¢˜ã€‚

**æŒ‘æˆ˜**ï¼š
- éç»“æ„åŒ–PDFæ–‡æ¡£ï¼ˆè´¢æŠ¥ã€æŠ¥å‘Šï¼‰
- è·¨æ–‡æ¡£æŸ¥è¯¢
- éœ€è¦æ•°å€¼è®¡ç®—å’Œå¯¹æ¯”åˆ†æ
- å¤šæ ¼å¼æ•°æ®ï¼ˆè¡¨æ ¼ã€æ–‡æœ¬ã€å›¾ç‰‡ï¼‰

### é¡¹ç›®å®ç°

#### Step 1: æ–‡æ¡£è§£æä¸é¢„å¤„ç†


```python
import os
from typing import List, Dict
from pathlib import Path
import fitz  # PyMuPDF
from unstructured.partition.pdf import partition_pdf

class InvestmentDocumentParser:
    """æŠ•èµ„æ–‡æ¡£è§£æå™¨"""

    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.extracted_docs = []

    def parse_pdf(self, file_path: str) -> Dict:
        """
        è§£æPDFæ–‡æ¡£ï¼Œæå–æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾ç‰‡
        """
        doc = fitz.open(file_path)
        result = {
            'filename': os.path.basename(file_path),
            'pages': [],
            'texts': [],
            'tables': [],
            'metadata': doc.metadata
        }

        for page_num in range(len(doc)):
            page = doc[page_num]

            # æå–æ–‡æœ¬
            text = page.get_text()
            result['texts'].append({
                'page': page_num + 1,
                'content': text,
                'type': 'text'
            })

            # --- ä¿®å¤éƒ¨åˆ†å¼€å§‹ ---
            # æå–è¡¨æ ¼ï¼ˆç®€å•æ–¹æ³•ï¼‰
            try:
                tables = page.find_tables()
                # å…³é”®ä¿®å¤ï¼šå¢åŠ  if tables åˆ¤æ–­ï¼Œé˜²æ­¢ tables ä¸º None æ—¶æŠ¥é”™
                if tables: 
                    for table in tables:
                        table_data = table.extract()
                        result['tables'].append({
                            'page': page_num + 1,
                            'data': table_data,
                            'type': 'table'
                        })
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: ç¬¬ {page_num + 1} é¡µè¡¨æ ¼æå–å¤±è´¥: {e}")
            # --- ä¿®å¤éƒ¨åˆ†ç»“æŸ ---

            # æå–å›¾ç‰‡
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                xref = img[0]
                try:
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]

                    result['texts'].append({
                        'page': page_num + 1,
                        'content': f"[å›¾ç‰‡ {img_index + 1}]",
                        'type': 'image',
                        'image_data': image_bytes
                    })
                except Exception as e:
                    print(f"âš ï¸ å›¾ç‰‡æå–è­¦å‘Š: {e}")

        doc.close()
        return result

    def process_directory(self) -> List[Dict]:
        """æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDF"""
        pdf_files = list(self.data_dir.glob("*.pdf"))
        print(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

        for pdf_file in pdf_files:
            print(f"æ­£åœ¨å¤„ç†: {pdf_file.name}")
            doc_info = self.parse_pdf(str(pdf_file))
            self.extracted_docs.append(doc_info)

        print(f"âœ… å®Œæˆå¤„ç†ï¼Œå…± {len(self.extracted_docs)} ä¸ªæ–‡æ¡£")
        return self.extracted_docs

    def save_extracted_text(self, output_dir: str):
        """ä¿å­˜æå–çš„æ–‡æœ¬"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        for doc in self.extracted_docs:
            # ä¿å­˜çº¯æ–‡æœ¬
            text_content = "\n".join([
                t['content'] for t in doc['texts']
            ])

            with open(output_path / f"{doc['filename']}.txt", 'w', encoding='utf-8') as f:
                f.write(text_content)

            # ä¿å­˜è¡¨æ ¼CSV
            if doc['tables']:
                import pandas as pd
                for i, table in enumerate(doc['tables']):
                    df = pd.DataFrame(table['data'])
                    df.to_csv(output_path / f"{doc['filename']}_table_{i}.csv", index=False, encoding='utf-8')

        print(f"âœ… æ–‡æœ¬å·²ä¿å­˜åˆ° {output_path}")
```

#### Step 2: æ„å»ºå¤šæ¨¡æ€RAGç³»ç»Ÿ


```python
from typing import List, Dict, Tuple
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

class MultimodalRAG:
    """å¤šæ¨¡æ€RAGç³»ç»Ÿï¼šæ”¯æŒæ–‡æœ¬å’Œè¡¨æ ¼"""

    def __init__(self):
        self.text_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.text_chunks = []
        self.table_chunks = []
        self.text_index = None
        self.table_index = None

    def index_texts(self, texts: List[str], metadata: List[Dict] = None):
        """ç´¢å¼•æ–‡æœ¬"""
        if metadata is None:
            metadata = [{}] * len(texts)

        self.text_chunks = [
            {
                'text': text,
                'metadata': meta,
                'type': 'text'
            }
            for text, meta in zip(texts, metadata)
        ]

        embeddings = self.text_model.encode(texts)
        self.text_index = faiss.IndexFlatIP(embeddings.shape[1])
        self.text_index.add(embeddings.astype('float32'))

        print(f"âœ… å·²ç´¢å¼• {len(texts)} ä¸ªæ–‡æœ¬å—")

    def index_tables(self, table_files: List[str]):
        """ç´¢å¼•è¡¨æ ¼"""
        table_texts = []
        table_metadata = []

        for file_path in table_files:
            df = pd.read_csv(file_path)
            # å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
            table_text = self._table_to_text(df, file_path)
            table_texts.append(table_text)
            table_metadata.append({
                'type': 'table',
                'file': file_path,
                'shape': df.shape
            })

        self.table_chunks = [
            {'text': text, 'metadata': meta, 'type': 'table'}
            for text, meta in zip(table_texts, table_metadata)
        ]

        # ç´¢å¼•è¡¨æ ¼æ–‡æœ¬
        embeddings = self.text_model.encode(table_texts)
        self.table_index = faiss.IndexFlatIP(embeddings.shape[1])
        self.table_index.add(embeddings.astype('float32'))

        print(f"âœ… å·²ç´¢å¼• {len(table_texts)} ä¸ªè¡¨æ ¼")

    def _table_to_text(self, df: pd.DataFrame, filename: str) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬æè¿°"""
        # ç”Ÿæˆè¡¨æ ¼çš„æ–‡æœ¬è¡¨ç¤º
        text_parts = [f"è¡¨æ ¼æ–‡ä»¶: {filename}"]

        # æ·»åŠ è¡¨å¤´
        text_parts.append(f"åˆ—å: {', '.join(df.columns.tolist())}")

        # æ·»åŠ å‰å‡ è¡Œæ•°æ®
        text_parts.append("æ•°æ®é¢„è§ˆ:")
        for i, row in df.head(10).iterrows():
            row_text = ", ".join([f"{col}: {val}" for col, val in row.items()])
            text_parts.append(row_text)

        return "\n".join(text_parts)

    def hybrid_search(
        self,
        query: str,
        k: int = 5,
        text_weight: float = 0.7,
        table_weight: float = 0.3
    ) -> List[Dict]:
        """æ··åˆæ£€ç´¢ï¼šæ–‡æœ¬ + è¡¨æ ¼"""
        query_embedding = self.text_model.encode([query])[0]

        results = []

        # æ–‡æœ¬æ£€ç´¢
        if self.text_index:
            text_scores, text_indices = self.text_index.search(
                query_embedding.reshape(1, -1).astype('float32'),
                k
            )
            for score, idx in zip(text_scores[0], text_indices[0]):
                results.append({
                    'chunk': self.text_chunks[idx],
                    'score': score * text_weight,
                    'source': 'text'
                })

        # è¡¨æ ¼æ£€ç´¢
        if self.table_index:
            table_scores, table_indices = self.table_index.search(
                query_embedding.reshape(1, -1).astype('float32'),
                k
            )
            for score, idx in zip(table_scores[0], table_indices[0]):
                results.append({
                    'chunk': self.table_chunks[idx],
                    'score': score * table_weight,
                    'source': 'table'
                })

        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)

        return results[:k]

    def analyze_company_comparison(
        self,
        query: str,
        company_a: str,
        company_b: str
    ) -> Dict:
        """åˆ†æä¸¤å®¶å…¬å¸å¯¹æ¯”"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = self.hybrid_search(query, k=10)

        # æå–ä¸¤å®¶å…¬å¸çš„ä¿¡æ¯
        company_a_info = []
        company_b_info = []

        for result in results:
            text = result['chunk']['text'].lower()
            if company_a.lower() in text:
                company_a_info.append(result)
            elif company_b.lower() in text:
                company_b_info.append(result)

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report = self._generate_comparison_report(
            query, company_a, company_b,
            company_a_info, company_b_info
        )

        return {
            'query': query,
            'company_a': company_a,
            'company_b': company_b,
            'company_a_results': company_a_info,
            'company_b_results': company_b_info,
            'comparison_report': report
        }

    def _generate_comparison_report(
        self,
        query: str,
        company_a: str,
        company_b: str,
        info_a: List,
        info_b: List
    ) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = f"ğŸ“Š {company_a} vs {company_b} å¯¹æ¯”åˆ†æ\n"
        report += f"æŸ¥è¯¢: {query}\n\n"

        report += f"ã€{company_a}ã€‘\n"
        if info_a:
            for result in info_a:
                report += f"- {result['chunk']['text'][:200]}...\n"
        else:
            report += "- æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n"

        report += f"\nã€{company_b}ã€‘\n"
        if info_b:
            for result in info_b:
                report += f"- {result['chunk']['text'][:200]}...\n"
        else:
            report += "- æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯\n"

        report += "\nå»ºè®®: æ‰‹åŠ¨æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ–‡æ¡£ä»¥è·å–è¯¦ç»†æ•°æ®"

        return report

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    parser = InvestmentDocumentParser("C:\\Users\\Admin\\Desktop\\code\\Live_And_Learn\\Agent_Learning\\Stage_3")
    docs = parser.process_directory()
    parser.save_extracted_text("extracted_texts")

    # è·å–æå–çš„æ–‡æœ¬æ–‡ä»¶
    text_files = list(Path("extracted_texts").glob("*.txt"))
    table_files = list(Path("extracted_texts").glob("*.csv"))

    # æ„å»ºRAG
    rag = MultimodalRAG()
    rag.index_texts([open(f, 'r', encoding='utf-8').read() for f in text_files[:10]])
    rag.index_tables(table_files)

    # æµ‹è¯•å¯¹æ¯”æŸ¥è¯¢
    comparison = rag.analyze_company_comparison(
        "è¥æ”¶å¢é•¿ç‡",
        "é˜¿é‡Œ",
        "è…¾è®¯"
    )

    print(comparison['comparison_report'])
```

    æ‰¾åˆ° 3 ä¸ªPDFæ–‡ä»¶
    æ­£åœ¨å¤„ç†: albb.pdf
    æ­£åœ¨å¤„ç†: anthropic.pdf
    find_tables: exception occurred: code=4: no font file for digest
    æ­£åœ¨å¤„ç†: tencent.pdf
    âœ… å®Œæˆå¤„ç†ï¼Œå…± 3 ä¸ªæ–‡æ¡£
    âœ… æ–‡æœ¬å·²ä¿å­˜åˆ° extracted_texts
    âœ… å·²ç´¢å¼• 3 ä¸ªæ–‡æœ¬å—
    âœ… å·²ç´¢å¼• 34 ä¸ªè¡¨æ ¼
    ğŸ“Š é˜¿é‡Œ vs è…¾è®¯ å¯¹æ¯”åˆ†æ
    æŸ¥è¯¢: è¥æ”¶å¢é•¿ç‡
    
    çœç•¥

    å»ºè®®: æ‰‹åŠ¨æŸ¥çœ‹æ£€ç´¢åˆ°çš„æ–‡æ¡£ä»¥è·å–è¯¦ç»†æ•°æ®
    
